#  [INSTRUMENT IDENTIFICATION INFORMED MULTI-TRACK MIXING](http://music.ece.drexel.edu/files/Navigation/Publications/Scott2013b.pdf)
**Author**: Scott, Jeffrey; Kim, Youngmoo E

**Year**: 2013
>**Abstract**: Although digital music production technology has become more accessible over the years, the tools are complex and often difﬁcult to navigate, resulting in a large learning curve for new users. This paper approaches the task of automated multi-track mixing from the perspective of applying common practices based on the instrument types present in a mixture. We apply basic principles to each track automatically, varying the parameters of gain, stereo panning, and coarse equalization. Assuming all instruments are known, a small listening evaluation is completed on the mixed tracks to validate the assumptions of the mixing model. This work represents an exploratory analysis into the efﬁcacy of a hierarchical approach to multi-track mixing using instrument class as a guide to processing techniques.

**Data Set**: [Weathervane Music](https://weathervanemusic.org/), [Sound on Sound](http://www.soundonsound.com/), [Multi-Track Dataset](http://music.ece.drexel.edu/research/AutoMix)

**Source Code**: Not availabe

**Demo**: [Demo](http://music.ece.drexel.edu/research/AutoMix)

#  [DEEP LEARNING AND INTELLIGENT AUDIO MIXING](https://www.researchgate.net/publication/330967800_Deep_Learning_and_Intelligent_Audio_Mixing)
**Author**: Ramırez, Marco A Martınez; Reiss, Joshua D

**Year**: 2017
>**Abstract**: Mixing multitrack audio is a crucial part of music production. With recent advances in machine learning techniques such as deep learning, it is of great importance to conduct research on the applications of these methods in the ﬁeld of automatic mixing. In this paper, we present a survey of intelligent audio mixing systems and their recent incorporation of deep neural networks. We propose to the community a research trajectory in the ﬁeld of deep learning applied to intelligent music production systems. We conclude with a proof of concept based on stem audio mixing as a contentbased transformation using a deep autoencoder.

**Data Set**: [The open multitrack testbed](http://multitrack.eecs.qmul.ac.uk/), [MedleyDB](https://medleydb.weebly.com/)

**Source Code**: Not availabe

**Demo**: Not availabe

#  [Deep Remix_ Remixing Musical Mixtures using a Convolutional Deep Neural Network](https://arxiv.org/abs/1505.00289)
**Author**: Simpson, AJR; Roma, G

**Year**: 2015
>**Abstract**: Audio source separation is a difficult machine learning problem and performance is measured by comparing extracted signals with the component source signals. However, if separation is motivated by the ultimate goal of re-mixing then complete separation is not necessary and hence separation difficulty and separation quality are dependent on the nature of the re-mix. Here, we use a convolutional deep neural network (DNN), trained to estimate 'ideal' binary masks for separating voice from music, to perform re-mixing of the vocal balance by operating directly on the individual magnitude components of the musical mixture spectrogram. Our results demonstrate that small changes in vocal gain may be applied with very little distortion to the ultimate re-mix. Our method may be useful for re-mixing existing mixes.

**Data Set**: [MedleyDB](https://medleydb.weebly.com/)

**Source Code**: Not availabe

**Demo**: Not availabe

#  [AUTOMATIC MUSIC PRODUCTION SYSTEM EMPLOYING PROBABILISTIC EXPERT SYSTEMS](http://www.aes.org/e-lib/browse.cfm?elib=15677)
**Author**: Gang, Ren; Bocko, Gregory; Lundberg, Justin; Headlam, Dave; Bocko, Mark F

**Year**: 2010
>**Abstract**: An automatic music production system based on expert audio engineering knowledge is proposed. An expert system based on a probabilistic graphical model is employed to embed professional audio engineering knowledge and infer automatic production decisions based on musical information extracted from audio files. The production pattern, which is represented as probabilistic graphical model, can be ‘learned’ from the operation data of a human audio engineer or manually constructed from domain knowledge. The authors also discuss the real-time implementation of the proposed automatic production system for live mixing application scenarios. Musical event alignment and prediction algorithms are introduced to improve the time synchronization performance of our production model. The authors conclude with performance evaluations and a brief summary.

**Data Set**: Not availabe

**Source Code**: Not availabe

**Demo**: Not availabe

#  [NEW SONORITIES FOR JAZZ RECORDINGS: SEPARATION AND MIXING USING DEEP NEURAL NETWORKS](https://pdfs.semanticscholar.org/d81b/9143a4574aa11bb9b2f40c56e7d343449855.pdf)
**Author**: Mimilakis, Stylianos Ioannis; Cano, Estefanıa; Abeßer, Jakob; Schuller, Gerald

**Year**: 2016
>**Abstract**: The audio mixing process is an art that has proven to be extremely hard to model: What makes a certain mix better than another one? How can the mixing processing chain be automatically optimized to obtain better results in a more efﬁcient manner? Over the last years, the scientiﬁc community has exploited methods from signal processing, music information retrieval, machine learning, and more recently, deep learning techniques to address these issues. In this work, a novel system based on deep neural networks (DNNs) is presented. It replaces the previously proposed steps of pitch-informed source separation and panoramabased remixing by an ensemble of trained DNNs.

**Data Set**: [Jazzomat dataset](https://jazzomat.hfm-weimar.de/dbformat/dbcontent.html)

**Source Code**: [Source Code](https://github.com/Js-Mim/aes_wimp)

**Demo**: [Demo](https://js-mim.github.io/aes_wimp/)

#  [Modeling of nonlinear audio effects with end-to-end deep neural networks](http://arxiv.org/abs/1810.06603)
**Author**: Ramirez, Marco A. Martínez; Reiss, Joshua D.

**Year**: 2018
>**Abstract**: In the context of music production, distortion effects are mainly used for aesthetic reasons and are usually applied to electric musical instruments. Most existing methods for nonlinear modeling are often either simpliﬁed or optimized to a very speciﬁc circuit. In this work, we investigate deep learning architectures for audio processing and we aim to ﬁnd a general purpose end-to-end deep neural network to perform modeling of nonlinear audio effects. We show the network modeling various nonlinearities and we discuss the generalization capabilities among different instruments.

**Data Set**: [IDMT-SMT-AudioEffects dataset](https://www.idmt.fraunhofer.de/en/business_units/m2d/smt/audio_effects.html)

**Source Code**: Not availabe

**Demo**: [Demo](https://github.com/mchijmma/modeling-nonlinear)

#  [Towards an Automated Multitrack Mixing Tool using Answer Set Programming](http://smc2017.aalto.fi/media/materials/proceedings/SMC17_p422.pdf)
**Author**: Everardo, Flavio

**Year**: 2017
>**Abstract**: The use of Answer Set Programming (ASP) inside musical domains has been demonstrated specially in composition, but so far it hasn’t overtaken engineering areas in studio-music (post) production such as multitrack mixing for stereo imaging. This article aims to demonstrate the use of this declarative approach to achieve a well-balanced mix. A knowledge base is compiled with rules and constraints extracted from the literature about what professional music producers and audio engineers suggest creating a good mix. More specially, this work can deliver either a mixed audio ﬁle (mixdown) as well as a mixing plan in (human-readable) text format, to serve as a starting point for producers and audio engineers to apply this methodology into their productions. Finally this article presents a decibel (dB) and a panning scale to explain how the mixes are generated.

**Data Set**: Not availabe

**Source Code**: Not availabe

**Demo**: [Demo](https://soundcloud.com/flavioeverardo-research/sets)

#  [AUTOMATIC MULTI-TRACK MIXING USING LINEAR DYNAMICAL SYSTEMS](http://music.ece.drexel.edu/files/Navigation/Publications/Scott2011a.pdf)
**Author**: Scott, Jeffrey; Prockup, Matthew; Schmidt, Erik M; Kim, Youngmoo E

**Year**: 2011
>**Abstract**: Over the past several decades music production has evolved from something that was only possible with multiroom, multi-million dollar studios into the province of the average person’s living room. New tools for digital production have revolutionized the way we consume and interact with music on a daily basis. We propose a system based on a structured audio framework that can generate a basic mix-down of a set of multi-track audio ﬁles using parameters learned through supervised machine learning. Given the new surge of mobile content consumption, we extend this system to operate on a mobile device as an initial measure towards an integrated interactive mixing platform for multi-track music.

**Data Set**: [RockBand Database](https://rbdb.online/)

**Source Code**: Not availabe

**Demo**: [Demo](http://music.ece.drexel.edu/research/AutoMix/lds)

#  [Towards a Semantic Web Representation and Application of Audio Mixing Rules](https://qmro.qmul.ac.uk/xmlui/handle/123456789/59960)
**Author**: Moffat, David; Thalmann, Florian; Sandler, Mark B

**Year**: 2018
>**Abstract**: Existing literature has discussed the use of rule-based systems for intelligent mixing. These rules can either be explicitly deﬁned by experts, learned from existing datasets, or a mixture of both. For such mixing rules to be transferable between different systems and shared online, we propose a representation using the Rule Interchange Format (RIF) commonly used on the Semantic Web. Systems with differing capabilities can use OWL reasoning on those mixing rule sets to determine subsets which they can handle appropriately. We demonstrate this by means of an example web-based tool which uses a logical constraint solver to apply the rules in real time to sets of audio tracks annotated with features.

**Data Set**: Not availabe

**Source Code**: [Source Code](https://github.com/dymic-music/dymo-core)

**Demo**: Not availabe

#  [A knowledge-engineered autonomous mixing system](http://www.aes.org/e-lib/browse.cfm?elib=17011)
**Author**: De Man, Brecht; Reiss, Joshua D.

**Year**: 2013
>**Abstract**: In this paper a knowledge-engineered mixing engine is introduced that uses semantic mixing rules and bases mixing decisions on instrument tags as well as elementary, low-level signal features. Mixing rules are derived from practical mixing engineering textbooks. The performance of the system is compared to existing automatic mixing tools as well as human engineers by means of a listening test, and future directions are established.

**Data Set**: Not availabe

**Source Code**: Not availabe

**Demo**: Not availabe

#  [Music remixing and upmixing using source separation](https://www.researchgate.net/publication/308058809_Music_remixing_and_upmixing_using_source_separation)
**Author**: Roma, Gerard; Grais, Emad M; Simpson, Andrew J R; Plumbley, Mark D

**Year**: 2016
>**Abstract**: Current research on audio source separation provides tools to estimate the signals contributed by different instruments in polyphonic music mixtures. Such tools can be already incorporated in music production and post-production workﬂows. In this paper, we describe recent experiments where audio source separation is applied to remixing and upmixing existing mono and stereo music content.

**Data Set**: [DSD100](https://sigsep.github.io/datasets/dsd100.html)

**Source Code**: Not availabe

**Demo**: Not availabe

