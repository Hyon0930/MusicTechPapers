Mother Group	Child Goup	Publication Year	Author	Title	Url	Abstract Note	Source code	Source code URL	Data Set Name 1	Data Set URL1	Data Set Nmae 2	Data Set URL 2	Data Set Name 3	Data Set URL 3	Demo	Demo URL	Demo 2	Demo 2 URL
Intelligent Music Production	Compressor	2019	Sheng, Di; Fazekas, Gyorgy	A Feature Learning Siamese Model for Intelligent Control of the Dynamic Range Compressor	https://arxiv.org/abs/1905.01022	In this paper, a siamese DNN model is proposed to learn the characteristics of the audio dynamic range compressor (DRC). This facilitates an intelligent control system that uses audio examples to conﬁgure the DRC, a widely used nonlinear audio signal conditioning technique in the areas of music production, speech communication and broadcasting. Several alternative siamese DNN architectures are proposed to learn feature embeddings that can characterise subtle effects due to dynamic range compression. These models are compared with each other as well as handcrafted features proposed in previous work. The evaluation of the relations between the hyperparameters of DNN and DRC parameters are also provided. The best model is able to produce a universal feature embedding that is capable of predicting multiple DRC parameters simultaneously, which is a signiﬁcant improvement from our previous research. The feature embedding shows better performance than handcrafted audio features when predicting DRC parameters for both monoinstrument audio loops and polyphonic music pieces.												
Intelligent Music Production	General Effects	2019	Ramírez, Marco A. Martínez; Benetos, Emmanouil; Reiss, Joshua D.	A general-purpose deep learning approach to model time-varying audio effects	http://arxiv.org/abs/1905.06148	Audio processors whose parameters are modiﬁed periodically over time are often referred as time-varying or modulation based audio effects. Most existing methods for modeling these type of effect units are often optimized to a very speciﬁc circuit and cannot be efﬁciently generalized to other time-varying effects. Based on convolutional and recurrent neural networks, we propose a deep learning architecture for generic black-box modeling of audio processors with long-term memory. We explore the capabilities of deep neural networks to learn such long temporal dependencies and we show the network modeling various linear and nonlinear, time-varying and time-invariant audio effects. In order to measure the performance of the model, we propose an objective metric based on the psychoacoustics of modulation frequency perception. We also analyze what the model is actually learning and how the given task is accomplished.	Source Code	https://github.com/lucieperrotta/ASP	IDMT-SMT-AudioEffects dataset	https://www.idmt.fraunhofer.de/en/business_units/m2d/smt/audio_effects.html					Demo	https://mchijmma.github.io/modeling-time-varying/		
Sound Source Separation	Summary	2019	Sawada, Hiroshi; Ono, Nobutaka; Kameoka, Hirokazu; Kitamura, Daichi; Saruwatari, Hiroshi	A review of blind source separation methods: two converging routes to ILRMA originating from ICA and NMF	https://www.cambridge.org/core/product/identifier/S2048770319000052/type/journal_article	This paper describes several important methods for the blind source separation of audio signals in an integrated manner. Two historically developed routes are featured. One started from independent component analysis and evolved to independent vector analysis (IVA) by extending the notion of independence from a scalar to a vector. In the other route, nonnegative matrix factorization (NMF) has been extended to multichannel NMF (MNMF). As a convergence point of these two routes, independent low-rank matrix analysis has been proposed, which integrates IVA and MNMF in a clever way. All the objective functions in these methods are efficiently optimized by majorization-minimization algorithms with appropriately designed auxiliary functions. Experimental results for a simple two-source two-microphone case are given to illustrate the characteristics of these five methods.												
Music Generation and Synthesis	Synthesis	2019	Donahue, Chris; McAuley, Julian; Puckette, Miller	ADVERSARIAL AUDIO SYNTHESIS	https://arxiv.org/abs/1802.04208	Audio signals are sampled at high temporal resolutions, and learning to synthesize audio requires capturing structure across a range of timescales. Generative adversarial networks (GANs) have seen wide success at generating images that are both locally and globally coherent, but they have seen little application to audio generation. In this paper we introduce WaveGAN, a ﬁrst attempt at applying GANs to unsupervised synthesis of raw-waveform audio. WaveGAN is capable of synthesizing one second slices of audio waveforms with global coherence, suitable for sound effect generation. Our experiments demonstrate that—without labels—WaveGAN learns to produce intelligible words when trained on a smallvocabulary speech dataset, and can also synthesize audio from other domains such as drums, bird vocalizations, and piano. We compare WaveGAN to a method which applies GANs designed for image generation on image-like audio feature representations, ﬁnding both approaches to be promising.	Source Code	https://github.com/chrisdohue/wavegan	Speech Commands Dataset	https://ai.googleblog.com/2017/08/launching-speech-commands-dataset.html	Peter Boesman. Bird recordings.	https://www.xeno-canto.org/contributor/OOECIWCSWV			Demo	https://chrisdohue.com/wavegan_examples/	Demo	https://chrisdohue.com/wavegan/
Music Generation and Synthesis	Synthesis	2019	Marafioti, Andrés; Holighaus, Nicki; Perraudin, Nathanaël; Majdak, Piotr	Adversarial Generation of Time-Frequency Features with application in audio synthesis	https://arxiv.org/abs/1902.04072	Time-frequency (TF) representations provide powerful and intuitive features for the analysis of time series such as audio. But still, generative modeling of audio in the TF domain is a subtle matter. Consequently, neural audio synthesis widely relies on directly modeling the waveform and previous attempts at unconditionally synthesizing audio from neurally generated invertible TF features still struggle to produce audio at satisfying quality. In this article, focusing on the short-time Fourier transform, we discuss the challenges that arise in audio synthesis based on generated invertible TF features and how to overcome them. We demonstrate the potential of deliberate generative TF modeling by training a generative adversarial network (GAN) on short-time Fourier features. We show that by applying our guidelines, our TF-based network was able to outperform a state-of-the-art GAN generating waveforms directly, despite the similar architecture in the two networks.												
Intelligent Music Production	Tool	2019	Ward, Dominic	Audio library for modelling loudness	https://github.com/deeuu/loudness		Source Code	https://github.com/deeuu/loudness										
Others	Others	2019	Roche, Fanny; Hueber, Thomas; Limier, Samuel; Girin, Laurent	Autoencoders for music sound modeling: a comparison of linear, shallow, deep, recurrent and variational models	https://arxiv.org/abs/1806.04096	This study investigates the use of non-linear unsupervised dimensionality reduction techniques to compress a music dataset into a low-dimensional representation which can be used in turn for the synthesis of new sounds. We systematically compare (shallow) autoencoders (AEs), deep autoencoders (DAEs), recurrent autoencoders (with Long ShortTerm Memory cells – LSTM-AEs) and variational autoencoders (VAEs) with principal component analysis (PCA) for representing the high-resolution short-term magnitude spectrum of a large and dense dataset of music notes into a lower-dimensional vector (and then convert it back to a magnitude spectrum used for sound resynthesis). Our experiments were conducted on the publicly available multiinstrument and multi-pitch database NSynth. Interestingly and contrary to the recent literature on image processing, we can show that PCA systematically outperforms shallow AE. Only deep and recurrent architectures (DAEs and LSTM-AEs) lead to a lower reconstruction error. The optimization criterion in VAEs being the sum of the reconstruction error and a regularization term, it naturally leads to a lower reconstruction accuracy than DAEs but we show that VAEs are still able to outperform PCA while providing a low-dimensional latent space with nice “usability” properties. We also provide corresponding objective measures of perceptual audio quality (PEMO-Q scores), which generally correlate well with the reconstruction error.			NSynth dataset	https://magenta.tensorflow.org/datasets/nsynth					Demo 1	http://www.gipsa-lab.fr/~fanny.roche/SMC_2019.html		
Intelligent Music Production	Level Adjustment	2019	Moffat, David; Sandler, Mark B	Automatic Mixing Level Balancing Enhanced Through Source Interference Identiﬁcation	http://www.aes.org/e-lib/browse.cfm?elib=20355	It has been well established that equal loudness normalisation can produce a perceptually appropriate level balance in an automated mix. Previous work assumes that each captured track represents an individual sound source. In the context of a live drum recording this assumption is incorrect. This paper will demonstrate approach to identify the source interference and adjust the source gains accordingly, to ensure that tracks are all set to equal perceptual loudness. The impact of this interference on the selected gain parameters and resultant mixture is highlighted.			ENST dataset	http://www.tsi.telecom-paristech.fr/aao/en/2010/02/19/enst-drums-an-extensive-audio-visual-database-for-drum-sigls-processing/								
Feature	Feature	2019	Purwins, Hendrik; Li, Bo; Virtanen, Tuomas; Schlüter, Jan; Chang, Shuo-yiin; Sainath, Tara	Deep Learning for Audio Signal Processing	http://arxiv.org/abs/1905.00078	Given the recent surge in developments of deep learning, this article provides a review of the state-of-the-art deep learning techniques for audio signal processing. Speech, music, and environmental sound processing are considered sideby-side, in order to point out similarities and differences between the domains, highlighting general methods, problems, key references, and potential for cross-fertilization between areas. The dominant feature representations (in particular, log-mel spectra and raw waveform) and deep learning models are reviewed, including convolutional neural networks, variants of the long short-term memory architecture, as well as more audio-speciﬁc neural network models. Subsequently, prominent deep learning application areas are covered, i.e. audio recognition (automatic speech recognition, music information retrieval, environmental sound detection, localization and tracking) and synthesis and transformation (source separation, audio enhancement, generative models for speech, sound, and music synthesis). Finally, key issues and future questions regarding deep learning applied to audio signal processing are identiﬁed.			Million Song Dataset	http://millionsongdataset.com/	AudioSet	https://research.google.com/audioset/						
Generic	Generic	2019	Purwins, Hendrik; Li, Bo; Virtanen, Tuomas; Schlüter, Jan; Chang, Shuo-yiin; Sainath, Tara	Deep Learning for Audio Signal Processing	http://arxiv.org/abs/1905.00078	Given the recent surge in developments of deep learning, this article provides a review of the state-of-the-art deep learning techniques for audio signal processing. Speech, music, and environmental sound processing are considered sideby-side, in order to point out similarities and differences between the domains, highlighting general methods, problems, key references, and potential for cross-fertilization between areas. The dominant feature representations (in particular, log-mel spectra and raw waveform) and deep learning models are reviewed, including convolutional neural networks, variants of the long short-term memory architecture, as well as more audio-speciﬁc neural network models. Subsequently, prominent deep learning application areas are covered, i.e. audio recognition (automatic speech recognition, music information retrieval, environmental sound detection, localization and tracking) and synthesis and transformation (source separation, audio enhancement, generative models for speech, sound, and music synthesis). Finally, key issues and future questions regarding deep learning applied to audio signal processing are identiﬁed.			Million Song Dataset	http://millionsongdataset.com/	AudioSet	https://research.google.com/audioset/						
Sound Source Separation	Music Audio Source Separation	2019	Lluís, Francesc; Pons, Jordi; Serra, Xavier	End-to-end music source separation: is it possible in the waveform domain?	http://arxiv.org/abs/1810.12187	Most of the currently successful source separation techniques use the magnitude spectrogram as input, and are therefore by default omitting part of the signal: the phase. To avoid omitting potentially useful information, we study the viability of using end-to-end models for music source separation — which take into account all the information available in the raw audio signal, including the phase. Although during the last decades end-to-end music source separation has been considered almost unattainable, our results conﬁrm that waveform-based models can perform similarly (if not better) than a spectrogram-based deep learning model. Namely: a Wavenet-based model we propose and Wave-U-Net can outperform DeepConvSep, a recent spectrogram-based deep learning model.	Source Code	https://github.com/francesclluis/source-separation-wavenet	DSD100	https://sigsep.github.io/datasets/dsd100.html					Demo	http://jordipons.me/apps/end-to-end-music-source-separation/		
Music Generation and Synthesis	Synthesis	2019	Engel, Jesse; Agrawal, Kumar Krishna; Chen, Shuo; Gulrajani, Ishaan; Donahue, Chris; Roberts, Adam	GANSYNTH: ADVERSARIAL NEURAL AUDIO SYNTHESIS	https://arxiv.org/abs/1902.08710	Efficient audio synthesis is an inherently difficult machine learning task, as human perception is sensitive to both global structure and fine-scale waveform coherence. Autoregressive models, such as WaveNet, model local structure but have slow iterative sampling and lack global latent structure. In contrast, Generative Adversarial Networks (GANs) have global latent conditioning and efficient parallel sampling, but struggle to generate locally-coherent audio waveforms. Herein, we demonstrate that GANs can in fact generate high-fidelity and locally-coherent audio by modeling log magnitudes and instantaneous frequencies with sufficient frequency resolution in the spectral domain. Through extensive empirical investigations on the NSynth dataset, we demonstrate that GANs are able to outperform strong WaveNet baselines on automated and human evaluation metrics, and efficiently generate audio several orders of magnitude faster than their autoregressive counterparts.	Source Code	https://github.com/tensorflow/magenta/tree/master/magenta/models/gansynth	The NSynth Dataset	https://magenta.tensorflow.org/datasets/nsynth					Demo	https://colab.research.google.com/notebooks/magenta/gansynth/gansynth_demo.ipynb	Demo	https://storage.googleapis.com/magentadata/papers/gansynth/index.html
Intelligent Music Production	Text Book	2019	Brecht De Man, Ryan Stables, Joshua D. Reiss	Intelligent Music Production	https://www.crcpress.com/Intelligent-Music-Production/Man-Stables-Reiss/p/book/9781138055193													
Text Book	Text Book	2019	Brecht De Man, Ryan Stables, Joshua D. Reiss	Intelligent Music Production	https://www.crcpress.com/Intelligent-Music-Production/Man-Stables-Reiss/p/book/9781138055193													
Sound Source Separation	Music Audio Source Separation	2019	Schulze, Sören; King, Emily J.	Sparse Pursuit and Dictionary Learning for Blind Source Separation in Polyphonic Music Recordings	http://arxiv.org/abs/1806.00273	We propose a novel method for the blind separation of single-channel audio signals produced by the mixed sounds of musical instruments. While the approach of applying non-negative matrix factorization (NMF) has been studied in many papers, it does not make use of the pitch-invariance that the sounds of many instruments exhibit. This limitation can be overcome by using tensor factorization, in which context the use of log-frequency spectrograms was initiated, but this still requires the specific tuning of the instruments to be hard-coded into the algorithm. We develop a general-purpose sparse pursuit method that matches a discrete spectrum with given shifted continuous patterns. We first use it in order to transform our audio signal into a log-frequency spectrogram that shares properties with the mel spectrogram but is applicable to a wider frequency range. Then, we use the same algorithm to identify patterns from instrument sounds in the spectrogram. The relative amplitudes of the harmonics are saved in a dictionary, which is trained via a modified version of Adam. For a realistic monaural piece with acoustic recorder and violin, we achieve qualitatively good separation with a signal-to-distortion ratio (SDR) of 13.7 dB, a signal-to-interference ratio (SIR) of 28.1 dB, and a signal-to-artifacts ratio (SAR) of 13.9 dB, averaged over the instruments.	Source Code	https://github.com/rgcda/Musisep							Demo	https://www.math.colostate.edu/~king/software.html#Musisep		
Audio Drama	Audio Drama	2019	Chourdakis, Emmanouil Theofanis; Ward, Lauren; Paradis, Matthew; Reiss, Joshua D	MODELLING EXPERTS’ DECISIONS ON ASSIGNING NARRATIVE IMPORTANCES OF OBJECTS IN A RADIO DRAMA MIX	https://www.researchgate.net/publication/335988097_MODELLING_EXPERTS'_DECISIONS_ON_ASSIGNING_NARRATIVE_IMPORTANCES_OF_OBJECTS_IN_A_RADIO_DRAMA_MIX	There is an increasing number of consumers of broadcast audio who suffer from a degree of hearing impairment. One of the meth- ods developed for tackling this issue consists of creating customiz- able object-based audio mixes where users can attenuate parts of the mix using a simple complexity parameter. The method relies on the mixing engineer classifying audio objects in the mix ac- cording to their narrative importance. This paper focuses on automating this process. Individual tracks are classified based on their music, speech, or sound effect content. Then the decisions for assigning narrative importance to each segment of a radio drama mix are modelled using mixture distributions. Finally, the learned decisions and resultant mixes are evaluated using the Short Term Objective Intelligibility, with ref- erence to the narrative importance selections made by the original producer. This approach has applications for providing customiz- able mixes for legacy content, or automatically generated media content where the engineer is not able to intervene.	Source Code	https://github.com/bbc/audio-dafx2019-automatic/	GTZAN Genre Collection	http://marsyas.info/downloads/datasets.html	BBC sound effects	http://bbcsfx.acropolis.org.uk/						
Music Generation and Synthesis	Generation	2018	Roberts, Adam; Engel, Jesse; Raffel, Colin; Hawthorne, Curtis; Eck, Douglas	A Hierarchical Latent Vector Model for Learning Long-Term Structure in Music	http://arxiv.org/abs/1803.05428	"The Variational Autoencoder (VAE) has proven to be an effective model for producing semantically meaningful latent representations for natural data. However, it has thus far seen limited application to sequential data, and, as we demonstrate, existing recurrent VAE models have difficulty modeling sequences with long-term structure. To address this issue, we propose the use of a hierarchical decoder, which first outputs embeddings for subsequences of the input and then uses these embeddings to generate each subsequence independently. This structure encourages the model to utilize its latent code, thereby avoiding the ""posterior collapse"" problem which remains an issue for recurrent VAEs. We apply this architecture to modeling sequences of musical notes and find that it exhibits dramatically better sampling, interpolation, and reconstruction performance than a ""flat"" baseline model. An implementation of our ""MusicVAE"" is available online at http://g.co/magenta/musicvae-code."	Source Code	https://github.com/tensorflow/magenta/tree/master/magenta/models/music_vae	Lakh MIDI Dataset	https://colinraffel.com/projects/lmd/					Demo	https://storage.googleapis.com/magentadata/papers/musicvae/index.html		
Music Generation and Synthesis	Translation	2018	Mor, Noam; Wolf, Lior; Polyak, Adam; Taigman, Yaniv	A Universal Music Translation Network	http://arxiv.org/abs/1805.07848	We present a method for translating music across musical instruments, genres, and styles. This method is based on a multi-domain wavenet autoencoder, with a shared encoder and a disentangled latent space that is trained end-to-end on waveforms. Employing a diverse training dataset and large net capacity, the domain-independent encoder allows us to translate even from musical domains that were not seen during training. The method is unsupervised and does not rely on supervision in the form of matched samples between domains or musical transcriptions. We evaluate our method on NSynth, as well as on a dataset collected from professional musicians, and achieve convincing translations, even when translating from whistling, potentially enabling the creation of instrumental music by untrained humans.			The NSynth Dataset	https://magenta.tensorflow.org/datasets/nsynth					Demo	https://www.youtube.com/watch?v=vdxCqNWTpUs&feature=youtu.be		
Sound Correction	Clipping	2018	Rencker, Lucas; Bach, Francis; Wang, Wenwu; Plumbley, Mark D.	Consistent Dictionary Learning for Signal Declipping	http://epubs.surrey.ac.uk/846156/1/Consistent_DL_for_signal_declipping.pdf	Clipping, or saturation, is a common nonlinear distortion in signal processing. Recently, declipping techniques have been proposed based on sparse decomposition of the clipped signals on a ﬁxed dictionary, with additional constraints on the amplitude of the clipped samples. Here we propose a dictionary learning approach, where the dictionary is directly learned from the clipped measurements. We propose a softconsistency metric that minimizes the distance to a convex feasibility set, and takes into account our knowledge about the clipping process. We then propose a gradient descent-based dictionary learning algorithm that minimizes the proposed metric, and is thus consistent with the clipping measurement. Experiments show that the proposed algorithm outperforms other dictionary learning algorithms applied to clipped signals. We also show that learning the dictionary directly from the clipped signals outperforms consistent sparse coding with a ﬁxed dictionary.	Source Code	https://www.cvssp.org/Persol/LucasRencker/software.html							Demo	https://www.cvssp.org/Persol/LucasRencker/software.html		
Sound Source Separation	Speech Sound Separation	2018	Mobin, Shariq; Cheung, Brian; Olshausen, Bruno	CONVOLUTIONAL VS. RECURRENT NEURAL NET- WORKS FOR AUDIO SOURCE SEPARATION	https://openreview.net/forum?id=SkKnhFJPG	We propose a convolutional neural network as an alternative to recurrent neural networks for separating out individual speakers in a sound mixture. Our results achieve state-of-the-art results with an order of magnitude fewer parameters. We also characterize the robustness of both models to generalize to three different testing conditions including a novel dataset. We create a new dataset RealTalkLibri which evaluates how well source separation models generalize to real world mixtures. Our results indicate the acoustics of the environment have signiﬁcant impact on the performance of all neural network models, with the convolutional model showing superior ability to generalize to new environments.			CSR-I (WSJ0) Complete	https://catalog.ldc.upenn.edu/LDC93S6A	RealTalkLibri	https://www.dropbox.com/s/4pscejhkqdr8xrk/rtl.tar.gz?dl=0						
Intelligent Music Production	Equalization	2018	Campos, Guilherme; Fonseca, Nuno; Ferreira, Anibal; Davies, Matthew	END-TO-END EQUALIZATION WITH CONVOLUTIONAL NEURAL NETWORKS	http://dafx2018.web.ua.pt/papers/DAFx2018_paper_27.pdf	This work aims to implement a novel deep learning architecture to perform audio processing in the context of matched equalization. Most existing methods for automatic and matched equalization show effective performance and their goal is to ﬁnd a respective transfer function given a frequency response. Nevertheless, these procedures require a prior knowledge of the type of ﬁlters to be modeled. In addition, ﬁxed ﬁlter bank architectures are required in automatic mixing contexts. Based on end-to-end convolutional neural networks, we introduce a general purpose architecture for equalization matching. Thus, by using an end-toend learning approach, the model approximates the equalization target as a content-based transformation without directly ﬁnding the transfer function. The network learns how to process the audio directly in order to match the equalized target audio. We train the network through unsupervised and supervised learning procedures. We analyze what the model is actually learning and how the given task is accomplished. We show the model performing matched equalization for shelving, peaking, lowpass and highpass IIR and FIR equalizers.			Salamander Grand Piano V3 dataset	https://musical-artifacts.com/artifacts/3								
Intelligent Music Production	General Mixing	2018	Ramirez, Marco A. Martínez; Reiss, Joshua D.	Modeling of nonlinear audio effects with end-to-end deep neural networks	http://arxiv.org/abs/1810.06603	In the context of music production, distortion effects are mainly used for aesthetic reasons and are usually applied to electric musical instruments. Most existing methods for nonlinear modeling are often either simpliﬁed or optimized to a very speciﬁc circuit. In this work, we investigate deep learning architectures for audio processing and we aim to ﬁnd a general purpose end-to-end deep neural network to perform modeling of nonlinear audio effects. We show the network modeling various nonlinearities and we discuss the generalization capabilities among different instruments.			IDMT-SMT-AudioEffects dataset	https://www.idmt.fraunhofer.de/en/business_units/m2d/smt/audio_effects.html					Demo	https://github.com/mchijmma/modeling-nonlinear		
Music Generation and Synthesis	Translation	2018	Huang, Cheng-Zhi Anna; Vaswani, Ashish; Uszkoreit, Jakob; Shazeer, Noam; Simon, Ian; Hawthorne, Curtis; Dai, Andrew M.; Hoffman, Matthew D.; Dinculescu, Monica; Eck, Douglas	Music Transformer	http://arxiv.org/abs/1809.04281	Music relies heavily on repetition to build structure and meaning. Self-reference occurs on multiple timescales, from motifs to phrases to reusing of entire sections of music, such as in pieces with ABA structure. The Transformer (Vaswani et al., 2017), a sequence model based on self-attention, has achieved compelling results in many generation tasks that require maintaining long-range coherence. This suggests that self-attention might also be well-suited to modeling music. In musical composition and performance, however, relative timing is critically important. Existing approaches for representing relative positional information in the Transformer modulate attention based on pairwise distance (Shaw et al., 2018). This is impractical for long sequences such as musical compositions since their memory complexity for intermediate relative information is quadratic in the sequence length. We propose an algorithm that reduces their intermediate memory requirement to linear in the sequence length. This enables us to demonstrate that a Transformer with our modiﬁed relative attention mechanism can generate minutelong compositions (thousands of steps, four times the length modeled in Oore et al. (2018)) with compelling structure, generate continuations that coherently elaborate on a given motif, and in a seq2seq setup generate accompaniments conditioned on melodies1. We evaluate the Transformer with our relative attention mechanism on two datasets, JSB Chorales and Piano-e-Competition, and obtain state-of-the-art results on the latter.			J.S. Bach chorales dataset	https://github.com/czhuang/JSB-Chorales-dataset	Piano-e-Competition dataset	http://www.piano-e-competition.com/			Demo	https://storage.googleapis.com/music-transformer/index.html		
Sound Source Separation	Singing Voice Separation	2018	Grais, Emad M.; Ward, Dominic; Plumbley, Mark D.	Raw Multi-Channel Audio Source Separation using Multi-Resolution Convolutional Auto-Encoders	http://arxiv.org/abs/1803.00702	Supervised multi-channel audio source separation requires extracting useful spectral, temporal, and spatial features from the mixed signals. The success of many existing systems is therefore largely dependent on the choice of features used for training. In this work, we introduce a novel multi-channel, multiresolution convolutional auto-encoder neural network that works on raw time-domain signals to determine appropriate multiresolution features for separating the singing-voice from stereo music. Our experimental results show that the proposed method can achieve multi-channel audio source separation without the need for hand-crafted features or any pre- or post-processing.	Source Code	https://cvssp.github.io/maruss-website/publications/Grais_2018.html	SiSEC-2016-MUS-task dataset	https://sisec.inria.fr/sisec-2016/								
Sound Correction	Clipping	2018	Záviška, Pavel; Rajmic, Pavel; Průša, Zdeněk; Veselý, Vítězslav	Revisiting Synthesis Model of Sparse Audio Declipper	http://arxiv.org/abs/1807.03612	The state of the art in audio declipping has currently been achieved by SPADE (SParse Audio DEclipper) algorithm by Kiti´c et al. Until now, the synthesis/sparse variant, S-SPADE, has been considered signiﬁcantly slower than its analysis/cosparse counterpart, A-SPADE. It turns out that the opposite is true: by exploiting a recent projection lemma, individual iterations of both algorithms can be made equally computationally expensive, while S-SPADE tends to require considerably fewer iterations to converge. In this paper, the two algorithms are compared across a range of parameters such as the window length, window overlap and redundancy of the transform. The experiments show that although S-SPADE typically converges faster, the average performance in terms of restoration quality is not superior to A-SPADE.	http://www.utko.feec.vutbr.cz/~rajmic/software/aspade_vs_sspade.zip											
Music Generation and Synthesis	Generation	2018	Dieleman, Sander	The challenge of realistic music generation: modelling raw audio at scale	https://arxiv.org/abs/1806.10474	Realistic music generation is a challenging task. When building generative models of music that are learnt from data, typically high-level representations such as scores or MIDI are used that abstract away the idiosyncrasies of a particular performance. But these nuances are very important for our perception of musicality and realism, so in this work we embark on modelling music in the raw audio domain. It has been shown that autoregressive models excel at generating raw audio waveforms of speech, but when applied to music, we ﬁnd them biased towards capturing local signal structure at the expense of modelling long-range correlations. This is problematic because music exhibits structure at many different timescales. In this work, we explore autoregressive discrete autoencoders (ADAs) as a means to enable autoregressive models to capture long-range correlations in waveforms. We ﬁnd that they allow us to unconditionally generate piano music directly in the raw audio domain, which shows stylistic consistency across tens of seconds.			solo piano music	https://bit.ly/2IPXoDu					Demo	https://drive.google.com/drive/folders/1NY3MTkOSodz_5eCkjtoHUGYSulR25QGU		
Intelligent Music Production	General Mixing	2018	Moffat, David; Thalmann, Florian; Sandler, Mark B	Towards a Semantic Web Representation and Application of Audio Mixing Rules	https://qmro.qmul.ac.uk/xmlui/handle/123456789/59960	Existing literature has discussed the use of rule-based systems for intelligent mixing. These rules can either be explicitly deﬁned by experts, learned from existing datasets, or a mixture of both. For such mixing rules to be transferable between different systems and shared online, we propose a representation using the Rule Interchange Format (RIF) commonly used on the Semantic Web. Systems with differing capabilities can use OWL reasoning on those mixing rule sets to determine subsets which they can handle appropriately. We demonstrate this by means of an example web-based tool which uses a logical constraint solver to apply the rules in real time to sets of audio tracks annotated with features.	Source Code	https://github.com/dymic-music/dymo-core										
Sound Source Separation	Singing Voice Separation	2018	Stoller, Daniel; Ewert, Sebastian; Dixon, Simon	WAVE-U-NET: A MULTI-SCALE NEURAL NETWORK FOR END-TO-END AUDIO SOURCE SEPARATION	https://arxiv.org/abs/1806.03185	Models for audio source separation usually operate on the magnitude spectrum, which ignores phase information and makes separation performance dependant on hyperparameters for the spectral front-end. Therefore, we investigate end-to-end source separation in the time-domain, which allows modelling phase information and avoids ﬁxed spectral transformations. Due to high sampling rates for audio, employing a long temporal input context on the sample level is difﬁcult, but required for high quality separation results because of long-range temporal correlations. In this context, we propose the Wave-U-Net, an adaptation of the U-Net to the one-dimensional time domain, which repeatedly resamples feature maps to compute and combine features at different time scales. We introduce further architectural improvements, including an output layer that enforces source additivity, an upsampling technique and a context-aware prediction framework to reduce output artifacts. Experiments for singing voice separation indicate that our architecture yields a performance comparable to a stateof-the-art spectrogram-based U-Net architecture, given the same data. Finally, we reveal a problem with outliers in the currently used SDR evaluation metrics and suggest reporting rank-based statistics to alleviate this problem.	Source Code	https://github.com/f90/Wave-U-Net	MUSDB	https://sigsep.github.io/datasets/musdb.html	CCMixter database	http://dig.ccmixter.org/						
Audio Drama	Audio Drama	2018	Chourdakis, Emmanouil Theofanis; Reiss, Joshua D	From my pen to your ears: automatic production of radio plays from unstructured story text	http://www.eecs.qmul.ac.uk/~josh/documents/2018/smc_2018_018.pdf	A radio play is a form of drama which exists in the acoustic domain and is usually consumed over broadcast radio. In this paper a method is proposed that, given a story in the form of unstructured text, produces a radio play that tells this story. First, information about characters, acting lines, and environments is retrieved from the text. The information extracted serves to generate a production script which can be used either by producers of radio-drama, or subsequently used to automatically generate the radio play as an audio ﬁle. The system is evaluated in two parts: precision, recall, and f1 scores are computed for the information retrieval part while multistimulus listening tests are used for subjective evaluation of the generated audio.	Source Code	https://code.soundsoftware.ac.uk/projects/										
Intelligent Music Production	Feature	2017	Ramírez, Marco A Martínez; Reiss, Joshua D	Analysis and prediction of the audio feature space when mixing raw recordings into individual stems	http://www.aes.org/e-lib/browse.cfm?elib=19245	Processing individual stems from raw recordings is one of the ﬁrst steps of multitrack audio mixing. In this work, we explore which set of low-level audio features are sufﬁcient to design a prediction model for this transformation. We extract a large set of audio features from bass, guitar, vocal and keys raw recordings and stems. We show that a procedure based on random forests classiﬁers can lead us to reduce signiﬁcantly the number of features and we use the selected audio features to train various multi-output regression models. Thus, we investigate stem processing as a content-based transformation, where the inherent content of raw recordings leads us to predict the change of feature values that occurred within the transformation.			The open multitrack testbed	http://multitrack.eecs.qmul.ac.uk/	MedleyDB	https://medleydb.weebly.com/						
Intelligent Music Production	Level Adjustment	2017	Ward, Dominic	Applications of Loudness Models in Audio Engineering	http://www.open-access.bcu.ac.uk/7228/1/PhD%20Thesis.pdf	This thesis investigates the application of perceptual models to areas of audio engineering, with a particular focus on music production. The goal was to establish efficient and practical tools for the measurement and control of the perceived loudness of musical sounds. Two types of loudness model were investigated: the single-band model and the multiband excitation pattern (EP) model. The heuristic single-band devices were designed to be simple but sufficiently effective for real-world application, whereas the multiband procedures were developed to give a reasonable account of a large body of psychoacoustic findings according to a functional model of the peripheral hearing system. The research addresses the extent to which current models of loudness generalise to musical instruments, and whether can they be successfully employed in music applications. The domain-specific disparity between the two types of model was first tackled by reducing the computational load of state-of-the-art EP models to allow for fast but low-error auditory signal processing. Two elaborate hearing models were analysed and optimised using musical instruments and speech as test stimuli. It was shown that, after significantly reducing the complexity of both procedures, estimates of global loudness, such as peak loudness, as well as the intermediate auditory representations can be preserved with high accuracy. Based on the optimisations, two real-time applications were developed: a binaural loudness meter and an automatic multitrack mixer. This second system was designed to work independently of the loudness measurement procedure, and therefore supports both linear and nonlinear models. This allowed for a single mixing device to be assessed using different loudness metrics and this was demonstrated by evaluating three configurations through subjective assessment. Unexpectedly, when asked to rate both the overall quality of a mix and the degree to which instruments were equally loud, listeners preferred mixes generated using heuristic single-band models over those produced using a multiband procedure. A series of more systematic listening tests were conducted to further investigate this finding. Subjective loudness matches of musical instruments commonly found in western popular music were collected to evaluate the performance of five published models. The results were in accord with the application-based assessment, namely that current EP procedures do not generalise well when estimating the relative loudness of musical sounds which have marked differences in spectral content. Model specific issues were identified relating to the calculation of spectral loudness summation (SLS) and the method used to determine the global-loudness percept of time-varying musical sounds; associated refinements were proposed. It was shown that a new multiband loudness model with a heuristic loudness transformation yields superior performance over existing methods. This supports the idea that a revised model of SLS is needed, and therefore that modification to this stage in existing psychoacoustic procedures is an essential step towards the goal of achieving real-world deployment.			Sound Quality Assessment Material	https://sound.media.mit.edu/resources/mpeg4/audio/sqam/								
Sound Source Separation	Music Audio Source Separation	2017	Luo, Yi; Chen, Zhuo; Hershey, John R.; Le Roux, Jonathan; Mesgarani, Nima	Deep clustering and conventional networks for music separation: Stronger together	http://ieeexplore.ieee.org/document/7952118/	Deep clustering is the ﬁrst method to handle general audio separation scenarios with multiple sources of the same type and an arbitrary number of sources, performing impressively in speaker-independent speech separation tasks. However, little is known about its effectiveness in other challenging situations such as music source separation. Contrary to conventional networks that directly estimate the source signals, deep clustering generates an embedding for each time-frequency bin, and separates sources by clustering the bins in the embedding space. We show that deep clustering outperforms conventional networks on a singing voice separation task, in both matched and mismatched conditions, even though conventional networks have the advantage of end-to-end training for best signal approximation, presumably because its more ﬂexible objective engenders better regularization. Since the strengths of deep clustering and conventional network architectures appear complementary, we explore combining them in a single hybrid network trained via an approach akin to multi-task learning. Remarkably, the combination signiﬁcantly outperforms either of its components.			DSD100	https://sigsep.github.io/datasets/dsd100.html								
Sound Source Separation	Music Audio Source Separation	2017	Han, Yoonchang; Kim, Jaehun; Lee, Kyogu	Deep convolutional neural networks for predominant instrument recognition in polyphonic music	http://arxiv.org/abs/1605.09507	Identifying musical instruments in polyphonic music recordings is a challenging but important problem in the ﬁeld of music information retrieval. It enables music search by instrument, helps recognize musical genres, or can make music transcription easier and more accurate. In this paper, we present a convolutional neural network framework for predominant instrument recognition in real-world polyphonic music. We train our network from ﬁxed-length music excerpts with a single-labeled predominant instrument and estimate an arbitrary number of predominant instruments from an audio signal with a variable length. To obtain the audio-excerpt-wise result, we aggregate multiple outputs from sliding windows over the test audio. In doing so, we investigated two different aggregation methods: one takes the average for each instrument and the other takes the instrument-wise sum followed by normalization. In addition, we conducted extensive experiments on several important factors that affect the performance, including analysis window size, identiﬁcation threshold, and activation functions for neural networks to ﬁnd the optimal set of parameters. Using a dataset of 10k audio excerpts from 11 instruments for evaluation, we found that convolutional neural networks are more robust than conventional methods that exploit spectral features and source separation with support vector machines. Experimental results showed that the proposed convolutional network architecture obtained an F1 measure of 0.602 for micro and 0.503 for macro, respectively, achieving 19.6% and 16.4% in performance improvement compared with other state-of-the-art algorithms.			RWC Music Database	https://staff.aist.go.jp/m.goto/RWC-MDB/								
Intelligent Music Production	General Mixing	2017	Ramırez, Marco A Martınez; Reiss, Joshua D	DEEP LEARNING AND INTELLIGENT AUDIO MIXING	https://www.researchgate.net/publication/330967800_Deep_Learning_and_Intelligent_Audio_Mixing	Mixing multitrack audio is a crucial part of music production. With recent advances in machine learning techniques such as deep learning, it is of great importance to conduct research on the applications of these methods in the ﬁeld of automatic mixing. In this paper, we present a survey of intelligent audio mixing systems and their recent incorporation of deep neural networks. We propose to the community a research trajectory in the ﬁeld of deep learning applied to intelligent music production systems. We conclude with a proof of concept based on stem audio mixing as a contentbased transformation using a deep autoencoder.			The open multitrack testbed	http://multitrack.eecs.qmul.ac.uk/	MedleyDB	https://medleydb.weebly.com/						
Intelligent Music Production	Level Adjustment	2017	Ward, Dominic; Wierstorf, Hagen; Mason, Russell D; Plumbley, Mark D; Hummersone, Chris	ESTIMATING THE LOUDNESS BALANCE OF MUSICAL MIXTURES USING AUDIO SOURCE SEPARATION	https://pdfs.semanticscholar.org/2810/4e5ab0375cb95d866112002952489b291742.pdf	To assist with the development of intelligent mixing systems, it would be useful to be able to extract the loudness balance of sources in an existing musical mixture. The relative-to-mix loudness level of four instrument groups was predicted using the sources extracted by 12 audio source separation algorithms. The predictions were compared with the ground truth loudness data of the original unmixed stems obtained from a recent dataset involving 100 mixed songs. It was found that the best source separation system could predict the relative loudness of each instrument group with an average root-mean-square error of 1.2 LU, with superior performance obtained on vocals.	Source Code	https://code.soundsoftware.ac.uk/hg/wimp17-ward-et-al	DSD100	https://sigsep.github.io/datasets/dsd100.html								
Sound Source Separation	Music Audio Source Separation	2017	Uhlich, Stefan; Porcu, Marcello; Giron, Franck; Enenkl, Michael; Kemp, Thomas; Takahashi, Naoya; Mitsufuji, Yuki	Improving music source separation based on deep neural networks through data augmentation and network blending	http://ieeexplore.ieee.org/document/7952158/	This paper deals with the separation of music into individual instrument tracks which is known to be a challenging problem. We describe two different deep neural network architectures for this task, a feed-forward and a recurrent one, and show that each of them yields themselves state-of-the art results on the SiSEC DSD100 dataset. For the recurrent network, we use data augmentation during training and show that even simple separation networks are prone to overﬁtting if no data augmentation is used. Furthermore, we propose a blending of both neural network systems where we linearly combine their raw outputs and then perform a multi-channel Wiener ﬁlter post-processing. This blending scheme yields the best results that have been reported to-date on the SiSEC DSD100 dataset.			DSD100	https://sigsep.github.io/datasets/dsd100.html								
Intelligent Music Production	Reverb	2017	Benito, Adán L; Reiss, Joshua D	Intelligent Multitrack Reverberation Based on Hinge-Loss Markov Random Fields	http://www.aes.org/e-lib/browse.cfm?elib=18766	We propose a machine learning approach based on hinge-loss Markov random ﬁelds to solve the problem of applying reverb automatically to a multitrack session. With the objective of obtaining perceptually meaningful results, a set of Probabilistic Soft Logic (PSL) rules has been deﬁned based on best practices recommended by experts. These rules have been weighted according to the level of conﬁdence associated with the mentioned practices based on existent evidence. The resulting model has been used to extract parameters for a series of reverb units applied over the different tracks to obtain a reverberated mix of the session.			The open multitrack testbed	http://multitrack.eecs.qmul.ac.uk/					Demo	https://code.soundsoftware.ac.uk/projects/multitrackreverb		
Sound Source Separation	Music Audio Source Separation	2017	Chandna, Pritish; Miron, Marius; Janer, Jordi; Gómez, Emilia	Monoaural Audio Source Separation Using Deep Convolutional Neural Networks	http://link.springer.com/10.1007/978-3-319-53547-0_25	In this paper we introduce a low-latency monaural source separation framework using a Convolutional Neural Network (CNN). We use a CNN to estimate time-frequency soft masks which are applied for source separation. We evaluate the performance of the neural network on a database comprising of musical mixtures of three instruments: voice, drums, bass as well as other instruments which vary from song to song. The proposed architecture is compared to a Multilayer Perceptron (MLP), achieving on-par results and a signiﬁcant improvement in processing time. The algorithm was submitted to source separation evaluation campaigns to test eﬃciency, and achieved competitive results.	Source Code	https://github.com/MTG/DeepConvSep	DSD100	https://sigsep.github.io/datasets/dsd100.html					Demo	https://www.youtube.com/watch?v=71WwHyDfE		
Music Generation and Synthesis	Generation	2017	Engel, Jesse; Resnick, Cinjon; Roberts, Adam; Dieleman, Sander; Eck, Douglas; Simonyan, Karen; Norouzi, Mohammad	Neural Audio Synthesis of Musical Notes with WaveNet Autoencoders	http://arxiv.org/abs/1704.01279	Generative models in vision have seen rapid progress due to algorithmic improvements and the availability of high-quality image datasets. In this paper, we oﬀer contributions in both these areas to enable similar progress in audio modeling. First, we detail a powerful new WaveNet-style autoencoder model that conditions an autoregressive decoder on temporal codes learned from the raw audio waveform. Second, we introduce NSynth, a large-scale and high-quality dataset of musical notes that is an order of magnitude larger than comparable public datasets. Using NSynth, we demonstrate improved qualitative and quantitative performance of the WaveNet autoencoder over a well-tuned spectral autoencoder baseline. Finally, we show that the model learns a manifold of embeddings that allows for morphing between instruments, meaningfully interpolating in timbre to create new types of sounds that are realistic and expressive.			The NSynth Dataset	https://magenta.tensorflow.org/datasets/nsynth								
Music Information Retrieval	Feature	2017	Shuvaev, Sergey; Giaffar, Hamza; Koulakov, Alexei A	Representations of Sound in Deep Learning of Audio Features from Music	https://arxiv.org/abs/1712.02898	The work of a single musician, group or composer can vary widely in terms of musical style. Indeed, different stylistic elements, from performance medium and rhythm to harmony and texture, are typically exploited and developed across an artist’s lifetime. Yet, there is often a discernable character to the work of, for instance, individual composers at the perceptual level – an experienced listener can often pick up on subtle clues in the music to identify the composer or performer. Here we suggest that a convolutional network may learn these subtle clues or features given an appropriate representation of the music. In this paper, we apply a deep convolutional neural network to a large audio dataset and empirically evaluate its performance on audio classification tasks. Our trained network demonstrates accurate performance on such classification tasks when presented with ~ 5 s examples of music obtained by simple transformations of the raw audio waveform. A particularly interesting example is the spectral representation of music obtained by application of a logarithmically spaced filter bank, mirroring the early stages of auditory signal transduction in mammals. The most successful representation of music to facilitate discrimination was obtained via a random matrix transform (RMT). Networks based on logarithmic filter banks and RMT were able to correctly guess the one composer out of 31 possibilities in 68% and 84% of cases respectively.												
Sound Source Separation	Music Audio Source Separation	2017	Grais, Emad M.; Plumbley, Mark D.	Single Channel Audio Source Separation using Convolutional Denoising Autoencoders	http://arxiv.org/abs/1703.08019	Deep learning techniques have been used recently to tackle the audio source separation problem. In this work, we propose to use deep fully convolutional denoising autoencoders (CDAEs) for monaural audio source separation. We use as many CDAEs as the number of sources to be separated from the mixed signal. Each CDAE is trained to separate one source and treats the other sources as background noise. The main idea is to allow each CDAE to learn suitable spectral-temporal ﬁlters and features to its corresponding source. Our experimental results show that CDAEs perform source separation slightly better than the deep feedforward neural networks (FNNs) even with fewer parameters than FNNs.			SiSEC-2015-MUS-task dataset	https://sisec.inria.fr/sisec-2015/								
Intelligent Music Production	Summary	2017	Stables, Ryan	Ten Years of Automatic Mixing	https://www.researchgate.net/publication/320196585_Ten_Years_of_Automatic_Mixing	Reﬂecting on a decade of Automatic Mixing systems for multitrack music processing, this paper positions the topic in the wider ﬁeld of Intelligent Music Production, and seeks to motivate the existing and continued work in this area. Tendencies such as the introduction of machine learning and the increasing complexity of automated systems become apparent from examining a short history of relevant work, and several categories of applications are identiﬁed. Based on this systematic review, we highlight some promising directions for future research for the next ten years of Automatic Mixing.												
Data Set	Data Set	2017	Man, Brecht De; Reiss, Joshua D	The Mix Evaluation Dataset	http://www.dafx17.eca.ed.ac.uk/papers/DAFx17_paper_49.pdf	Research on perception of music production practices is mainly concerned with the emulation of sound engineering tasks through lab-based experiments and custom software, sometimes with unskilled subjects. This can improve the level of control, but the validity, transferability, and relevance of the results may suffer from this artiﬁcial context. This paper presents a dataset consisting of mixes gathered in a real-life, ecologically valid setting, and perceptual evaluation thereof, which can be used to expand knowledge on the mixing process. With 180 mixes including parameter settings, close to 5000 preference ratings and free-form descriptions, and a diverse range of contributors from ﬁve different countries, the data offers many opportunities for music production analysis, some of which are explored here. In particular, more experienced subjects were found to be more negative and more speciﬁc in their assessments of mixes, and to increasingly agree with each other.			The Mix Evaluation Dataset	http://c4dm.eecs.qmul.ac.uk/multitrack/MixEvaluation/								
Intelligent Music Production	General Mixing	2017	Everardo, Flavio	Towards an Automated Multitrack Mixing Tool using Answer Set Programming	http://smc2017.aalto.fi/media/materials/proceedings/SMC17_p422.pdf	The use of Answer Set Programming (ASP) inside musical domains has been demonstrated specially in composition, but so far it hasn’t overtaken engineering areas in studio-music (post) production such as multitrack mixing for stereo imaging. This article aims to demonstrate the use of this declarative approach to achieve a well-balanced mix. A knowledge base is compiled with rules and constraints extracted from the literature about what professional music producers and audio engineers suggest creating a good mix. More specially, this work can deliver either a mixed audio ﬁle (mixdown) as well as a mixing plan in (human-readable) text format, to serve as a starting point for producers and audio engineers to apply this methodology into their productions. Finally this article presents a decibel (dB) and a panning scale to explain how the mixes are generated.									Demo	https://soundcloud.com/flavioeverardo-research/sets		
Music Information Retrieval	Chord Detection	2016	Korzeniowski, Filip; Widmer, Gerhard	A Fully Convolutional Deep Auditory Model for Musical Chord Recognition	http://arxiv.org/abs/1612.05082	Chord recognition systems depend on robust feature extraction pipelines. While these pipelines are traditionally hand-crafted, recent advances in end-to-end machine learning have begun to inspire researchers to explore data-driven methods for such tasks. In this paper, we present a chord recognition system that uses a fully convolutional deep auditory model for feature extraction. The extracted features are processed by a Conditional Random Field that decodes the ﬁnal chord sequence. Both processing stages are trained automatically and do not require expert knowledge for optimising parameters. We show that the learned auditory system extracts musically interpretable features, and that the proposed chord recognition system achieves results on par or better than state-of-the-art algorithms.			Isophonic	http://isophonics.net/datasets	RWC Music Database	https://staff.aist.go.jp/m.goto/RWC-MDB/			Demo	https://www.music-ir.org/mirex/wiki/MIREX_HOME		
Sound Correction	Denosing	2016	Lusterman, Ethan	A Partitioned Autoencoder for Audio De-Noising	http://ee.cooper.edu/~keene/assets/lusterman_thesis.pdf	In this thesis, we introduce a modified partitioned autoencoder for de-noising audio without access to clean data for training. Traditional linear timeinvariant (LTI) systems such as the Wiener filter rely on power spectral density (PSD) estimates of desired signals and noise signals, which require some knowledge of the ground truth signals. One nonlinear approach in this area includes the use of de-noising autoencoders, which are one form of artificial neural networks (ANN). The nonlinearity of neural networks allow for more complex models to be made than LTI models. However, since de-noising autoencoders also require access to clean data and knowledge of the noise corruption process, we build on existing literature for a semi-supervised partitioned autoencoder that can perform de-noising without the clean signals during training. We compare existing semi-supervised de-noising systems as well as canonical supervised de-noising autoencoders. We show that for moderate levels of noise, our autoencoder outperforms existing schemes.												
Intelligent Music Production	Equalization	2016	Välimäki, Vesa; Reiss, Joshua	All About Audio Equalization: Solutions and Frontiers	http://www.mdpi.com/2076-3417/6/5/129	Audio equalization is a vast and active research area. The extent of research means that one often cannot identify the preferred technique for a particular problem. This review paper bridges those gaps, systemically providing a deep understanding of the problems and approaches in audio equalization, their relative merits and applications. Digital signal processing techniques for modifying the spectral balance in audio signals and applications of these techniques are reviewed, ranging from classic equalizers to emerging designs based on new advances in signal processing and machine learning. Emphasis is placed on putting the range of approaches within a common mathematical and conceptual framework. The application areas discussed herein are diverse, and include well-deﬁned, solvable problems of ﬁlter design subject to constraints, as well as newly emerging challenges that touch on problems in semantics, perception and human computer interaction. Case studies are given in order to illustrate key concepts and how they are applied in practice. We also recommend preferred signal processing approaches for important audio equalization problems. Finally, we discuss current challenges and the uncharted frontiers in this ﬁeld. The source code for methods discussed in this paper is made available at https://code.soundsoftware.ac.uk/projects/allaboutaudioeq.	Source Code	https://code.soundsoftware.ac.uk/projects/allaboutaudioeq	SocialEQ	http://www.markcartwright.com/research/socialeq								
Sound Correction	Clipping	2016	Dukes, Bob	An Efficient Algorithm for Clipping Detection and Declipping Audio	http://www.aes.org/e-lib/browse.cfm?elib=18486	We present an algorithm for end to end declipping, which includes clipping detection and the replacement of clipped samples. To detect regions of clipping, we analyze the signal’s amplitude histogram and the shape of the signal in the time-domain. The sample replacement algorithm uses a two-pass approach: short regions of clipping are replaced in the time-domain and long regions of clipping are replaced in the frequency-domain. The algorithm is robust against different types of clipping and is efficient compared to existing approaches. The algorithm has been implemented in an open source JavaScript client-side web application. Clipping detection is shown to give an f-measure of 0.92 and is robust to the clipping level.			Music Audio Benchmark Data Set	https://www-ai.cs.tu-dortmund.de/audio.html								
Intelligent Music Production	Evolutional computation	2016	Wilson, Alex; Fazenda, Bruno	AN EVOLUTIONARY COMPUTATION APPROACH TO INTELLIGENT MUSIC PRODUCTION INFORMED BY EXPERIMENTALLY GATHERED DOMAIN KNOWLEDGE	https://usir.salford.ac.uk/id/eprint/44278/1/Wilson.pdf	A novel methodology for intelligent music production has been developed using evolutionary computation. Mixes are generated by exploration of a “mix-space”, which consists of a series of inter-channel volume ratios, allowing efﬁcient generation of random mixes. An interactive genetic algorithm was used, allowing the user to rate mixes and guide the system towards their ideal mix. Currently, ﬁtness evaluation is subjective but can be aided by speciﬁc domain knowledge obtained from a large-scale study of real mixes.												
Intelligent Music Production	Reverb	2016	Chourdakis, Emmanouil Theofanis; Reiss, Joshua D	Automatic Control of a Digital Reverberation Effect using Hybrid Models	http://www.aes.org/e-lib/browse.cfm?elib=18090	Adaptive Digital Audio Effects are sound transformations controlled by features extracted from the sound itself. Artificial reverberation is used by sound engineers in the mixing process for a variety of technical and artistic reasons, including to give the perception that it was captured in a closed space. We propose a design of an adaptive digital audio effect for artificial reverberation that allows it to learn from the user in a supervised way. We perform feature selection and dimensionality reduction on features extracted from our training data set. Then a user provides examples of reverberation parameters for the training data. Finally, we train a set of classifiers and compare them using 10-fold cross validation to compare classification success ratios and mean squared errors. Tracks from the Open Multitrack Testbed are used in order to train and test our models.			The open multitrack testbed	http://multitrack.eecs.qmul.ac.uk/								
Intelligent Music Production	Level Adjustment	2016	Ward, Dominic; Reiss, Joshua D	LOUDNESS ALGORITHMS FOR AUTOMATIC MIXING	https://pdfs.semanticscholar.org/75c8/bdb6432ee547d3a167badaa128151c242366.pdf	Accurate loudness measurement is imperative for intelligent music mixing systems, where one of the most fundamental tasks is to automate the fader balance. The goal of this short paper is to highlight state-of-the-art loudness algorithms to the automatic mixing community, and give insight into their differences when applied to multi-track audio.	Source Code	https://code.soundsoftware.ac.uk/hg/wimp16-ward-reiss										
Intelligent Music Production	Tool	2016	Ward, Dominic	Loudness meters for Digital Audio Workstations	https://github.com/deeuu/LoudnessMeters		Source Code	https://github.com/deeuu/LoudnessMeters										
Sound Correction	Phase Correction	2016	Magron, Paul; Badeau, Roland; David, Bertrand	Model-based STFT phase recovery for audio source separation	http://arxiv.org/abs/1608.01953	This paper introduces a novel technique for reconstructing the phase of modiﬁed spectrograms of audio signals. From the analysis of mixtures of sinusoids we obtain relationships between phases of successive time frames in the Time-Frequency (TF) domain. Instantaneous frequencies are estimated locally to encompass the class of non-stationary signals such as vibratos. This technique ensures the horizontal coherence (over time) of the partials. The method is tested on a variety of data and demonstrates better performance than traditional consistencybased approaches. We also introduce an audio restoration framework and obtain results that compete with other state-of-theart methods. Finally, we apply this phase recovery method to an audio source separation task where the spectrograms of the isolated components are known. We propose to use the phase unwrapping estimate to initialize a source separation iterative procedure. Experiments conducted on realistic music pieces demonstrate the effectiveness of such a method for various music signal processing tasks.			DSD100	https://sigsep.github.io/datasets/dsd100.html	MAPS - A piano database for multipitch estimation and automatic transcription of music,	http://www.tsi.telecom-paristech.fr/aao/en/2010/07/08/maps-database-a-piano-database-for-multipitch-estimation-and-automatic-transcription-of-music/						
Sound Source Separation	Music Audio Source Separation	2016	Magron, Paul; Badeau, Roland; David, Bertrand	Model-based STFT phase recovery for audio source separation	http://arxiv.org/abs/1608.01953	This paper introduces a novel technique for reconstructing the phase of modiﬁed spectrograms of audio signals. From the analysis of mixtures of sinusoids we obtain relationships between phases of successive time frames in the Time-Frequency (TF) domain. Instantaneous frequencies are estimated locally to encompass the class of non-stationary signals such as vibratos. This technique ensures the horizontal coherence (over time) of the partials. The method is tested on a variety of data and demonstrates better performance than traditional consistencybased approaches. We also introduce an audio restoration framework and obtain results that compete with other state-of-theart methods. Finally, we apply this phase recovery method to an audio source separation task where the spectrograms of the isolated components are known. We propose to use the phase unwrapping estimate to initialize a source separation iterative procedure. Experiments conducted on realistic music pieces demonstrate the effectiveness of such a method for various music signal processing tasks.			DSD100	https://sigsep.github.io/datasets/dsd100.html	MAPS - A piano database for multipitch estimation and automatic transcription of music,	http://www.tsi.telecom-paristech.fr/aao/en/2010/07/08/maps-database-a-piano-database-for-multipitch-estimation-and-automatic-transcription-of-music/						
Intelligent Music Production	Reverb	2016	Valimaki, Vesa; Parker, Julian D; Savioja, Lauri; Smith, Julius O; Abel, Jonathan S	More Than Fifty Years of Artiﬁcial Reverberation	http://www.aes.org/e-lib/browse.cfm?elib=18061	Recent research related to artificial reverberation is reviewed. The focus is on research published during the past few years after the writing of an overview article on the same topic by the same authors. Advances in delay networks, convolution-based techniques, physical room models, and virtual analog reverberation models are described. Many new developments are related to the feedback delay network, which is still the most popular parametric reverberation method. Additionally, three specific methods are discussed in detail: velvet-noise reverberation methods, scattering delay networks, and a modal architecture for artificial reverberation. It becomes evident that research on artificial reverberation and related topics continues to be as active as ever.												
Intelligent Music Production	General Mixing	2016	Roma, Gerard; Grais, Emad M; Simpson, Andrew J R; Plumbley, Mark D	Music remixing and upmixing using source separation	https://www.researchgate.net/publication/308058809_Music_remixing_and_upmixing_using_source_separation	Current research on audio source separation provides tools to estimate the signals contributed by different instruments in polyphonic music mixtures. Such tools can be already incorporated in music production and post-production workﬂows. In this paper, we describe recent experiments where audio source separation is applied to remixing and upmixing existing mono and stereo music content.			DSD100	https://sigsep.github.io/datasets/dsd100.html								
Intelligent Music Production	General Mixing	2016	Mimilakis, Stylianos Ioannis; Cano, Estefanıa; Abeßer, Jakob; Schuller, Gerald	NEW SONORITIES FOR JAZZ RECORDINGS: SEPARATION AND MIXING USING DEEP NEURAL NETWORKS	https://pdfs.semanticscholar.org/d81b/9143a4574aa11bb9b2f40c56e7d343449855.pdf	The audio mixing process is an art that has proven to be extremely hard to model: What makes a certain mix better than another one? How can the mixing processing chain be automatically optimized to obtain better results in a more efﬁcient manner? Over the last years, the scientiﬁc community has exploited methods from signal processing, music information retrieval, machine learning, and more recently, deep learning techniques to address these issues. In this work, a novel system based on deep neural networks (DNNs) is presented. It replaces the previously proposed steps of pitch-informed source separation and panoramabased remixing by an ensemble of trained DNNs.	Source Code	https://github.com/Js-Mim/aes_wimp	Jazzomat dataset	https://jazzomat.hfm-weimar.de/dbformat/dbcontent.html					Demo	https://js-mim.github.io/aes_wimp/		
Sound Source Separation	Summary	2016	Sell, Connor; Kepner, Jeremy	Non-Negative Matrix Factorization Test Cases	http://arxiv.org/abs/1701.00016	Non-negative matrix factorization (NMF) is a problem with many applications, ranging from facial recognition to document clustering. However, due to the variety of algorithms that solve NMF, the randomness involved in these algorithms, and the somewhat subjective nature of the problem, there is no clear “correct answer” to any particular NMF problem, and as a result, it can be hard to test new algorithms. This paper suggests some test cases for NMF algorithms derived from matrices with enumerable exact non-negative factorizations and perturbations of these matrices. Three algorithms using widely divergent approaches to NMF all give similar solutions over these test cases, suggesting that these test cases could be used as test cases for implementations of these existing NMF algorithms as well as potentially new NMF algorithms. This paper also describes how the proposed test cases could be used in practice.												
Music Generation and Synthesis	Generation	2016	Hadjeres, Gaëtan; Sakellariou, Jason; Pachet, François	Style Imitation and Chord Invention in Polyphonic Music with Exponential Families	http://arxiv.org/abs/1609.05152	Modeling polyphonic music is a particularly challenging task because of the intricate interplay between melody and harmony. A good model should satisfy three requirements: statistical accuracy (capturing faithfully the statistics of correlations at various ranges, horizontally and vertically), ﬂexibility (coping with arbitrary user constraints), and generalization capacity (inventing new material, while staying in the style of the training corpus). Models proposed so far fail on at least one of these requirements. We propose a statistical model of polyphonic music, based on the maximum entropy principle. This model is able to learn and reproduce pairwise statistics between neighboring note events in a given corpus. The model is also able to invent new chords and to harmonize unknown melodies. We evaluate the invention capacity of the model by assessing the amount of cited, re-discovered, and invented chords on a corpus of Bach chorales. We discuss how the model enables the user to specify and enforce user-deﬁned constraints, which makes it useful for style-based, interactive music generation.									Demo	https://flowmachines.jimdo.com/		
Music Information Retrieval	Feature	2016	Wilson, Alex; Fazenda, Bruno	Variation in Multitrack Mixes: Analysis of Low-level Audio Signal Features	http://www.aes.org/e-lib/browse.cfm?elib=18332	To further the development of intelligent music production tools towards generating mixes that would realistically be created by a human mix-engineer, it is important to understand what kind of mixes can be created, and are typically created, by human mix-engineers. This paper presents an analysis of 1501 mixes, over 10 different songs, created by mix-engineers. The primary dimensions of variation in the full dataset of mixes were “amplitude,” “brightness,” “bass,” and “width” as determined by feature-extraction and subsequent principal component analysis. The distribution of representative features approximated a normal distribution and this is then used to obtain general trends and tolerance bounds for these features. The results presented here are useful as parametric guidance for intelligent music production systems.			Cambridge Multitracks	http://www.cambridge-mt.com								
Music Generation and Synthesis	Generation	2016	Oord, Aaron van den; Dieleman, Sander; Zen, Heiga; Simonyan, Karen; Vinyals, Oriol; Graves, Alex; Kalchbrenner, Nal; Senior, Andrew; Kavukcuoglu, Koray	WaveNet: A Generative Model for Raw Audio	http://arxiv.org/abs/1609.03499	This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efﬁciently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-ofthe-art performance, with human listeners rating it as signiﬁcantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal ﬁdelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we ﬁnd that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.	Source Code	https://www.deepmind.com/blog/article/wavenet-generative-model-raw-audio	The MagTagATune dataset	http://mirg.city.ac.uk/codeapps/the-magtagatune-dataset					Demo	https://www.deepmind.com/blog/article/wavenet-generative-model-raw-audio		
Others	Others	2015	Gatys, Leon A.; Ecker, Alexander S.; Bethge, Matthias	A Neural Algorithm of Artistic Style	http://arxiv.org/abs/1508.06576	In fine art, especially painting, humans have mastered the skill to create unique visual experiences through composing a complex interplay between the content and style of an image. Thus far the algorithmic basis of this process is unknown and there exists no artificial system with similar capabilities. However, in other key areas of visual perception such as object and face recognition near-human performance was recently demonstrated by a class of biologically inspired vision models called Deep Neural Networks. Here we introduce an artificial system based on a Deep Neural Network that creates artistic images of high perceptual quality. The system uses neural representations to separate and recombine content and style of arbitrary images, providing a neural algorithm for the creation of artistic images. Moreover, in light of the striking similarities between performance-optimised artificial neural networks and biological vision, our work offers a path forward to an algorithmic understanding of how humans create and perceive artistic imagery.												
Sound Source Separation	Feature	2015	Deleforge, Antoine; Forbes, Florence; Horaud, Radu	Acoustic Space Learning for Sound Source Separation and Localization on Binaural Manifolds	http://arxiv.org/abs/1402.2683	In this paper we address the problems of modeling the acoustic space generated by a full-spectrum sound source and of using the learned model for the localization and separation of multiple sources that simultaneously emit sparse-spectrum sounds. We lay theoretical and methodological grounds in order to introduce the binaural manifold paradigm. We perform an in-depth study of the latent low-dimensional structure of the high-dimensional interaural spectral data, based on a corpus recorded with a human-like audiomotor robot head. A non-linear dimensionality reduction technique is used to show that these data lie on a two-dimensional (2D) smooth manifold parameterized by the motor states of the listener, or equivalently, the sound source directions. We propose a probabilistic piecewise affine mapping model (PPAM) specifically designed to deal with high-dimensional data exhibiting an intrinsic piecewise linear structure. We derive a closed-form expectation-maximization (EM) procedure for estimating the model parameters, followed by Bayes inversion for obtaining the full posterior density function of a sound source direction. We extend this solution to deal with missing data and redundancy in real world spectrograms, and hence for 2D localization of natural sound sources such as speech. We further generalize the model to the challenging case of multiple sound sources and we propose a variational EM framework. The associated algorithm, referred to as variational EM for source separation and localization (VESSL) yields a Bayesian estimation of the 2D locations and time-frequency masks of all the sources. Comparisons of the proposed approach with several existing methods reveal that the combination of acoustic-space learning with Bayesian inference enables our method to outperform state-of-the-art methods.			TIMIT Acoustic-Phonetic Continuous Speech Corpus	https://catalog.ldc.upenn.edu/LDC93S1	The CAMIL Dataset	http://humavips.inrialpes.fr/2011/02/11/project-events/index.html			Demo	https://team.inria.fr/perception/projects/popeye/		
Intelligent Music Production	Level Adjustment	2015		Algorithms to measure audio programme loudness and true-peak audio level	https://www.itu.int/dms_pubrec/itu-r/rec/bs/R-REC-BS.1770-4-201510-I!!PDF-E.pdf													
Music Information Retrieval	Feature	2015	Moffat, David; Ronan, David; Reiss, Joshua D	An Evaluation of Audio Feature Extraction Toolboxes	https://www.ntnu.edu/documents/1001201110/1266017954/DAFx-15_submission_43_v2.pdf	Audio feature extraction underpins a massive proportion of audio processing, music information retrieval, audio effect design and audio synthesis. Design, analysis, synthesis and evaluation often rely on audio features, but there are a large and diverse range of feature extraction tools presented to the community. An evaluation of existing audio feature extraction libraries was undertaken. Ten libraries and toolboxes were evaluated with the Cranﬁeld Model for evaluation of information retrieval systems, reviewing the coverage, effort, presentation and time lag of a system. Comparisons are undertaken of these tools and example use cases are presented as to when toolboxes are most suitable. This paper allows a software engineer or researcher to quickly and easily select a suitable audio feature extraction toolbox.			The million song dataset	http://millionsongdataset.com/								
Intelligent Music Production	Equalization	2015	Hafezi, Sina; Reiss, Joshua	Autonomous Multitrack Equalization Based on Masking Reduction	http://www.aes.org/e-lib/browse.cfm?elib=17637	Spectral masking is when the threshold of audibility for one sound is raised by the simultaneous presence of another sound. In multitrack music production, this results in less ability to fully hear and distinguish the sound sources in the mix. We design a simplified measure of masking based on best practices in sound engineering. We implement both off-line and realtime, low latency autonomous multitrack equalization systems to reduce masking in multitrack audio. We perform objective measurement of the spectral masking in the resultant mixes and conduct a listening test for subjective comparison between the mix results of different implementations of our system, a raw mix, and manual mixes made by an amateur and a professional mix engineer. The results show that autonomous systems reduce both the perceived masking and objective spectral masking and improve the overall quality of the mix. We show that our offline semi-autonomous system is capable of improving the raw mix better than an amateur and close to a professional mix by simply controlling one user parameter. Our results also suggest that existing objective measures of masking are ill-suited for quantifying perceived masking in multitrack musical audio.												
Intelligent Music Production	Level Adjustment	2015	Wichern, Gordon; Wishnick, Aaron S; Lukin, Alexey; Robertson, Hannah	Comparison of Loudness Features for Automatic Level Adjustment in Mixing	http://www.aes.org/e-lib/browse.cfm?elib=17928	Manually setting the level of each track of a multitrack recording is often the ﬁrst step in the mixing process. In order to automate this process, loudness features are computed for each track and gains are algorithmically adjusted to achieve target loudness values. In this paper we ﬁrst examine human mixes from a multitrack dataset to determine instrument-dependent target loudness templates. We then use these templates to develop three diﬀerent automatic level-based mixing algorithms. The ﬁrst is based on a simple energy-based loudness model, the second uses a more sophisticated psychoacoustic model, and the third incorporates masking eﬀects into the psychoacoustic model. The three automatic mixing approaches are compared to human mixes using a subjective listening test. Results show that subjects preferred the automatic mixes created from the simple energy-based model, indicating that the complex psychoacoustic model may not be necessary in an automated level setting application.			MedleyDB	https://medleydb.weebly.com/								
Sound Source Separation	Speech Sound Separation	2015	Hershey, John R.; Chen, Zhuo; Roux, Jonathan Le; Watanabe, Shinji	Deep clustering: Discriminative embeddings for segmentation and separation	http://arxiv.org/abs/1508.04306	We address the problem of acoustic source separation in a deep learning framework we call “deep clustering”. Rather than directly estimating signals or masking functions, we train a deep network to produce spectrogram embeddings that are discriminative for partition labels given in training data. Previous deep network approaches provide great advantages in terms of learning power and speed, but previously it has been unclear how to use them to separate signals in a classindependent way. In contrast, spectral clustering approaches are ﬂexible with respect to the classes and number of items to be segmented, but it has been unclear how to leverage the learning power and speed of deep networks. To obtain the best of both worlds, we use an objective function that to train embeddings that yield a low-rank approximation to an ideal pairwise afﬁnity matrix, in a classindependent way. This avoids the high cost of spectral factorization and instead produces compact clusters that are amenable to simple clustering methods. The segmentations are therefore implicitly encoded in the embeddings, and can be ”decoded” by clustering. Preliminary experiments show that the proposed method can separate speech: when trained on spectrogram features containing mixtures of two speakers, and tested on mixtures of a held-out set of speakers, it can infer masking functions that improve signal quality by around 6dB. We show that the model can generalize to three-speaker mixtures despite training only on twospeaker mixtures. The framework can be used without class labels, and therefore has the potential to be trained on a diverse set of sound types, and to generalize to novel sources. We hope that future work will lead to segmentation of arbitrary sounds, with extensions to microphone array methods as well as image segmentation and other domains.	Source Code	https://sourceforge.net/projects/currennt/	CSR-I (WSJ0) Complete	https://catalog.ldc.upenn.edu/LDC93S6A								
Intelligent Music Production	General Mixing	2015	Simpson, AJR; Roma, G	Deep Remix_ Remixing Musical Mixtures using a Convolutional Deep Neural Network	https://arxiv.org/abs/1505.00289	Audio source separation is a difficult machine learning problem and performance is measured by comparing extracted signals with the component source signals. However, if separation is motivated by the ultimate goal of re-mixing then complete separation is not necessary and hence separation difficulty and separation quality are dependent on the nature of the re-mix. Here, we use a convolutional deep neural network (DNN), trained to estimate 'ideal' binary masks for separating voice from music, to perform re-mixing of the vocal balance by operating directly on the individual magnitude components of the musical mixture spectrogram. Our results demonstrate that small changes in vocal gain may be applied with very little distortion to the ultimate re-mix. Our method may be useful for re-mixing existing mixes.			MedleyDB	https://medleydb.weebly.com/								
Sound Correction	Clipping	2015	Harvilla, Mark J.; Stern, Richard M.	Efficient audio declipping using regularized least squares	http://ieeexplore.ieee.org/document/7177964/	While many recently proposed audio declipping algorithms are highly effective in their ability to restore clipped speech, the algorithms’ computational complexities inhibit their use in many practical situations. Real-time or nearly real-time performance is impossible using a typical laptop computer, with some algorithms taking as long as 400 times the actual duration of the input to complete restoration. This paper introduces a novel declipping algorithm, referred to as Regularized Blind Amplitude Reconstruction, which is capable of restoring clipped audio at rates much faster than real time and at restoration qualities comparable to existing algorithms. The quality of declipping is evaluated in terms of automatic speech recognition performance on declipped speech, as well as the degree to which each declipping algorithm improves the audio’s signal-to-noise ratio.			Resource Magement Database	http://www.speech.cs.cmu.edu/databases/rm1/								
Text Book	Text Book	2015	Müller, Meinard	Fundamentals of Music Processing	http://link.springer.com/10.1007/978-3-319-21945-5													
Intelligent Music Production	Level Adjustment	2015	Durr, Gabriel; Peixoto, Lys; Souza, Marcelo; Tanoue, Raisa; Reiss, Joshua D	IMPLEMENTATION AND EVALUATION OF DYNAMIC LEVEL OF AUDIO DETAIL	https://www.eecs.qmul.ac.uk/~josh/documents/2015/Durr%20et%20al%20-%20AES56%20-%202015.pdf	Sound synthesis involves creating a desired sound using software or algorithms and analysing the digital signal processing involved in the creation of the sound, rather than recording it. However, synthesis techniques are often too computationally complex for use in many scenarios. This project aims to implement and assess sound synthesis models with dynamic Level of Audio Detail (LOAD). The manipulations consist of modifying existing models to achieve a more or less complex implementation while still retaining the perceptual characteristics of the sound. The models implemented consist of sine waves, noise sources and filters that reproduce the desired sound, which could then be enhanced or reduced to provide dynamic LOAD. These different levels were then analysed in the time-frequency domain, and computational time and floating point operations were assessed as a function of the LOAD.												
Sound Correction	Clipping	2015	Kitić, Srđan; Bertin, Nancy; Gribonval, Rémi	Sparsity and Cosparsity for Audio Declipping: A Flexible Non-convex Approach	https://arxiv.org/abs/1506.01830	This work investigates the empirical performance of the sparse synthesis versus sparse analysis regularization for the ill-posed inverse problem of audio declipping. We develop a versatile non-convex heuristics which can be readily used with both data models. Based on this algorithm, we report that, in most cases, the two models perform almost similarly in terms of signal enhancement. However, the analysis version is shown to be amenable for real time audio processing, when certain analysis operators are considered. Both versions outperform state-of-the-art methods in the ﬁeld, especially for the severely saturated signals.			RWC music database	https://staff.aist.go.jp/m.goto/RWC-MDB/								
Intelligent Music Production	Panning	2014	Pestana, Pedro D; Reiss, Joshua D	A CROSS-ADAPTIVE DYNAMIC SPECTRAL PANNING TECHNIQUE	https://www.eecs.qmul.ac.uk/~josh/documents/2014/Pestana%20Reiss%20-%20DAFx%20-%202014.pdf	This work presents an algorithm that is able to achieve novel spatialization effects on multitrack audio signals. It relies on a crossadaptive framework that dynamically maps the azimuth positions of each track’s time-frequency bins with the goal of reducing masking between source signals by dynamically separating them across space. The outputs of this system are compared to traditional panning strategies in subjective evaluation, and it is seen that scores indicate it performs well as a novel effect that can be used in live sound applications and creative sound design or mixing.									Demo	http://www.stereosonic.org/phd/specPanning/		
Sound Correction	Clipping	2014	Kitić, Srđan; Bertin, Nancy; Gribonval, Rémi	Audio Declipping by Cosparse Hard Thresholding	https://pdfs.semanticscholar.org/938f/24eb12c66bf4d8f81ddcbbecb9ce268e6b45.pdf?_ga=2.194532077.1935390015.1574141093-1556478679.1573971532	Recovery of clipped audio signals is a very challenging inverse problem. Recently, it has been successfully addressed by several methods based on the sparse synthesis data model. In this work we propose an algorithm for enhancement of clipped audio signals that exploits the sparse analysis (cosparse) data model. Experiments on real audio data indicate that the algorithm has better signal restoration performance than state-of-the-art sparse synthesis declipping methods.									Demo	https://spade.inria.fr/		
Sound Source Separation	Summary	2014	Jang, Giljin; Kim, Han-Gyu; Oh, Yung-Hwan	Audio Source Separation Using a Deep Autoencoder	http://arxiv.org/abs/1412.7193	This paper proposes a novel framework for unsupervised audio source separation using a deep autoencoder. The characteristics of unknown source signals mixed in the mixed input is automatically by properly conﬁgured autoencoders implemented by a network with many layers, and separated by clustering the coefﬁcient vectors in the code layer. By investigating the weight vectors to the ﬁnal target, representation layer, the primitive components of the audio signals in the frequency domain are observed. By clustering the activation coefﬁcients in the code layer, the previously unknown source signals are segregated. The original source sounds are then separated and reconstructed by using code vectors which belong to different clusters. The restored sounds are not perfect but yield promising results for the possibility in the success of many practical applications.												
Music Information Retrieval	Genre Classification	2014	Roche, Fanny; Hueber, Thomas; Limier, Samuel; Girin, Laurent	Finding musical genre similarity using machine learning techniques	https://www.cs.ru.nl/bachelors-theses/2014/Laurens_van_Dijk___3012557___Finding_musical_genre_similarity_using_machine_learning_techniques.pdf	This study investigates the use of non-linear unsupervised dimensionality reduction techniques to compress a music dataset into a low-dimensional representation which can be used in turn for the synthesis of new sounds. We systematically compare (shallow) autoencoders (AEs), deep autoencoders (DAEs), recurrent autoencoders (with Long ShortTerm Memory cells – LSTM-AEs) and variational autoencoders (VAEs) with principal component analysis (PCA) for representing the high-resolution short-term magnitude spectrum of a large and dense dataset of music notes into a lower-dimensional vector (and then convert it back to a magnitude spectrum used for sound resynthesis). Our experiments were conducted on the publicly available multiinstrument and multi-pitch database NSynth. Interestingly and contrary to the recent literature on image processing, we can show that PCA systematically outperforms shallow AE. Only deep and recurrent architectures (DAEs and LSTM-AEs) lead to a lower reconstruction error. The optimization criterion in VAEs being the sum of the reconstruction error and a regularization term, it naturally leads to a lower reconstruction accuracy than DAEs but we show that VAEs are still able to outperform PCA while providing a low-dimensional latent space with nice “usability” properties. We also provide corresponding objective measures of perceptual audio quality (PEMO-Q scores), which generally correlate well with the reconstruction error.	Source Code	https://www.cs.ru.nl/bachelors-theses/2014/Laurens_van_Dijk___3012557___Finding_musical_genre_similarity_using_machine_learning_techniques.pdf										
Intelligent Music Production	Summary	2014	Pestana, Pedro; Reiss, Joshua D	Intelligent Audio Production Strategies Informed by Best Practices	http://www.aes.org/e-lib/browse.cfm?elib=17121	The main focus of this article is to explore and investigate the fundamental constraints that should be at the basis of algorithm development in intelligent audio production systems. Through mix analysis and grounded theory strategies, a best-practices framework on the craft of mixing is sought out. Findings, while not to be taken as dogmatic, give a clear indication of preferred implementation strategies, and show what still needs to be done to fully understand the technical choices that audio mixing has incorporated throughout its history.												
Data Set	Data Set	2014	Bittner, Rachel; Salamon, Justin; Tierney, Mike; Mauch, Matthias; Cannam, Chris; Bello, Juan	MedleyDB: A MULTITRACK DATASET FOR ANNOTATION-INTENSIVE MIR RESEARCH	http://www.terasoft.com.tw/conf/ismir2014/proceedings/T028_322_Paper.pdf	We introduce MedleyDB: a dataset of annotated, royaltyfree multitrack recordings. The dataset was primarily developed to support research on melody extraction, addressing important shortcomings of existing collections. For each song we provide melody f0 annotations as well as instrument activations for evaluating automatic instrument recognition. The dataset is also useful for research on tasks that require access to the individual tracks of a song such as source separation and automatic mixing. In this paper we provide a detailed description of MedleyDB, including curation, annotation, and musical content. To gain insight into the new challenges presented by the dataset, we run a set of experiments using a state-of-the-art melody extraction algorithm and discuss the results. The dataset is shown to be considerably more challenging than the current test sets used in the MIREX evaluation campaign, thus opening new research avenues in melody extraction research.	Source Code	https://github.com/marl/medleydb	MedleyDB	https://medleydb.weebly.com/								
Sound Source Separation	Music Audio Source Separation	2014	Cano, Estefanía; Schuller, Gerald; Dittmar, Christian	Pitch-informed solo and accompaniment separation towards its use in music education applications	https://asp-eurasipjournals.springeropen.com/articles/10.1186/1687-6180-2014-23	We present a system for the automatic separation of solo instruments and music accompaniment in polyphonic music recordings. Our approach is based on a pitch detection front-end and a tone-based spectral estimation. We assess the plausibility of using sound separation technologies to create practice material in a music education context. To better understand the sound separation quality requirements in music education, a listening test was conducted to determine the most perceptually relevant signal distortions that need to be improved. Results from the listening test show that solo and accompaniment tracks pose different quality requirements and should be optimized differently. We propose and evaluate algorithm modifications to better understand their effects on objective perceptual quality measures. Finally, we outline possible ways of optimizing our separation approach to better suit the requirements of music education applications.			TRIOS Dataset	https://c4dm.eecs.qmul.ac.uk/rdr/handle/123456789/27	SISEC	http://sisec.wiki.irisa.fr/tiki-index.php?page=Professiolly%20produced%20music%20recordings	CCMixter database	http://dig.ccmixter.org/				
Intelligent Music Production	Level Adjustment	2014	Terrell,, Michael; Simpson, Andrew; Sandler, Mark	The Mathematics of Mixing	http://www.aes.org/e-lib/browse.cfm?elib=17081	Mixing is a quintessential optimization problem. Given control of several component tracks, a balance must be struck that reflects a trade-off between engineering methods, artistic objectives, and auditory perceptual constraints. Formally, this balance can be thought of as the optimal solution to a system of mathematical equations that describe the relationships between the component tracks within a mix. Hence, the nature of these equations defines the process by which solutions may be arrived at. As perception is strongly nonlinear, an analytical solution to this set of equations is not possible and so a search must be conducted. Here, taking loudness as an example, we develop an optimization theory treatment of the problem of mixing, complete with case studies to illustrate how auditory perception can complicate the process, not least due to masking-related interactions.												
Intelligent Music Production	General Mixing	2013	De Man, Brecht; Reiss, Joshua D.	A knowledge-engineered autonomous mixing system	http://www.aes.org/e-lib/browse.cfm?elib=17011	In this paper a knowledge-engineered mixing engine is introduced that uses semantic mixing rules and bases mixing decisions on instrument tags as well as elementary, low-level signal features. Mixing rules are derived from practical mixing engineering textbooks. The performance of the system is compared to existing automatic mixing tools as well as human engineers by means of a listening test, and future directions are established.												
Intelligent Music Production	Level Adjustment	2013	Mimilakis, Stylianos-Ioannis; Drossos, Konstantinos; Floros, Andreas; Katerelos, Dionysios	Automated Tonal Balance Enhancement for Audio Mastering Applications	http://www.aes.org/e-lib/browse.cfm?elib=16737	Modern audio mastering procedures are involved with the selective enhancement or attenuation of specific frequency bands. The main reason is the tonal enhancement of the original / unmastered audio material. The aforementioned process is mostly based on the musical information and the mode of the audio material. This information can be retrieved from a listening procedure of the original stimuli, or the corespondent musical key notes. The current work presents an adaptive and automated equalization system that performs the aforementioned mastering procedure, based on a novel method of fundamental frequency tracking. In addition to this, the overall system is being evaluated with objective PEAQ analysis and subjective listening tests in real mastering audio conditions.												
Sound Correction	Clipping	2013	Kitić, Srdjan; Jacques, Laurent; Madhu, Nilesh; Hopwood, Michael Peter; Spriet, Ann; De Vleeschouwer, Christophe	Consistent Iterative Hard Thresholding For Signal Declipping	http://arxiv.org/abs/1303.1023	Clipping or saturation in audio signals is a very common problem in signal processing, for which, in the severe case, there is still no satisfactory solution. In such case, there is a tremendous loss of information, and traditional methods fail to appropriately recover the signal. We propose a novel approach for this signal restoration problem based on the framework of Iterative Hard Thresholding. This approach, which enforces the consistency of the reconstructed signal with the clipped observations, shows superior performance in comparison to the state-of-the-art declipping algorithms. This is conﬁrmed on synthetic and on actual high-dimensional audio data processing, both on SNR and on subjective user listening evaluations.												
Tool	Tool	2013	Bogdanov, Dmitry; Wack, Nicolas; Gomez, Emilia; Gulati, Sankalp; Herrera, Perfecto; Mayor, Oscar; Roma, Gerard; Salamon, Justin; Zapata, Jose; Serra, Xavier	ESSENTIA: AN AUDIO ANALYSIS LIBRARY FOR MUSIC INFORMATION RETRIEVAL	http://www.justinsalamon.com/uploads/4/3/9/4/4394963/bogdanov_essentia_ismir13.pdf	We present Essentia 2.0, an open-source C++ library for audio analysis and audio-based music information retrieval released under the Affero GPL license. It contains an extensive collection of reusable algorithms which implement audio input/output functionality, standard digital signal processing blocks, statistical characterization of data, and a large set of spectral, temporal, tonal and high-level music descriptors. The library is also wrapped in Python and includes a number of predeﬁned executable extractors for the available music descriptors, which facilitates its use for fast prototyping and allows setting up research experiments very rapidly. Furthermore, it includes a Vamp plugin to be used with Sonic Visualiser for visualization purposes. The library is cross-platform and currently supports Linux, Mac OS X, and Windows systems. Essentia is designed with a focus on the robustness of the provided music descriptors and is optimized in terms of the computational cost of the algorithms. The provided functionality, speciﬁcally the music descriptors included in-the-box and signal processing algorithms, is easily expandable and allows for both research experiments and development of large-scale industrial applications.	Source Code	https://essentia.upf.edu/documentation/										
Intelligent Music Production	Equalization	2013	Ma, Zheng; Reiss, Joshua D; Black, Dawn A A	Implementation of an intelligent equalization tool using Yule-Walker for music mixing and mastering	http://www.aes.org/e-lib/browse.cfm?elib=16792	A new approach for automatically equalizing an audio signal towards a target frequency spectrum is presented. The algorithm is based on the Yule-Walker method and designs recursive IIR digital filters using Least-Squares fitting to any desired frequency response. The target equalization curve is obtained from the spectral distribution analysis of a large dataset of popular commercial recordings. A real-time C++ VST plug-in and an off-line Matlab implementation have been created. Straightforward objective evaluation is provided, where the output frequency spectra are compared against the target equalization curve and the ones produced by an alternative equalization method.												
Intelligent Music Production	General Mixing	2013	Scott, Jeffrey; Kim, Youngmoo E	INSTRUMENT IDENTIFICATION INFORMED MULTI-TRACK MIXING	http://music.ece.drexel.edu/files/Navigation/Publications/Scott2013b.pdf	Although digital music production technology has become more accessible over the years, the tools are complex and often difﬁcult to navigate, resulting in a large learning curve for new users. This paper approaches the task of automated multi-track mixing from the perspective of applying common practices based on the instrument types present in a mixture. We apply basic principles to each track automatically, varying the parameters of gain, stereo panning, and coarse equalization. Assuming all instruments are known, a small listening evaluation is completed on the mixed tracks to validate the assumptions of the mixing model. This work represents an exploratory analysis into the efﬁcacy of a hierarchical approach to multi-track mixing using instrument class as a guide to processing techniques.			Weathervane Music	https://weathervanemusic.org/	Sound on Sound	http://www.soundonsound.com/	Multi-Track Dataset	http://music.ece.drexel.edu/research/AutoMix	Demo	http://music.ece.drexel.edu/research/AutoMix		
Intelligent Music Production	Equalization	2013	Shekar, Priyanka; Smith, Julius	Modeling the Harmonic Exciter	http://www.aes.org/e-lib/browse.cfm?elib=16939	A harmonic exciter is an audio eﬀects signal processor applied to enhance the brightness and clarity of a sound, particularly used for vocals. This is achieved by inducing a measured amount of high-frequency distortion. In this paper, an exciter is digitally modeled and implemented as a standalone application (or plugin) using the FAUST (Functional AUdio STream) programming language for real-time audio. The model is based on the Aural Exciter by Aphex Ltd., an analog hardware unit. Technical speciﬁcations of the Aural Exciter are drawn from the original 1979 patent. The digital model performs as expected, recreating a “vintage” style audio eﬀect.	Source Code	https://github.com/grame-cncm/faust										
Intelligent Music Production	Compressor	2013	Giannoulis, Dimitrios; Massberg, Michael; Reiss, Joshua D.	Parameter Automation in a Dynamic Range Compressor	http://www.aes.org/e-lib/browse.cfm?elib=16965	Dynamic range compression is a nonlinear, time dependent audio effect. As such, preferred parameter settings are difficult to achieve even when there is advance knowledge of the input signal and the desired perceptual characteristics of the output. We introduce an automated approach to dynamic range compression where the parameters are configured automatically based on real-time, side-chain feature extraction from the input signal. Parameters are all dynamically varied depending on extracted features, leaving only the threshold as a user controlled parameter to set the preferred amount of compression. We analyze a series of automation techniques, including comparison of methods based on different signal characteristics. Subjective evaluation was performed with amateur and professional sound engineers, which established preference for dynamic range compressor parameters when applied to musical signals, and allowed us to compare performance of our various approaches against manual parameter settings.	Source Code	http://c4dm.eecs.qmul.ac.uk/audioengineering/compressors/										
Sound Correction	Denosing	2013	Lu, Xugang; Tsao, Yu; Matsuda, Shigeki; Hori, Chiori	Speech Enhancement Based on Deep Denoising Autoencoder	https://www.isca-speech.org/archive/archive_papers/interspeech_2013/i13_0436.pdf	We previously have applied deep autoencoder (DAE) for noise reduction and speech enhancement. However, the DAE was trained using only clean speech. In this study, by using noisyclean training pairs, we further introduce a denoising process in learning the DAE. In training the DAE, we still adopt greedy layer-wised pretraining plus ﬁne tuning strategy. In pretraining, each layer is trained as a one-hidden-layer neural autoencoder (AE) using noisy-clean speech pairs as input and output (or transformed noisy-clean speech pairs by preceding AEs). Fine tuning was done by stacking all AEs with pretrained parameters for initialization. The trained DAE is used as a ﬁlter for speech estimation when noisy speech is given. Speech enhancement experiments were done to examine the performance of the trained denoising DAE. Noise reduction, speech distortion, and perceptual evaluation of speech quality (PESQ) criteria are used in the performance evaluations. Experimental results show that adding depth of the DAE consistently increase the performance when a large training data set is given. In addition, compared with a minimum mean square error based speech enhancement algorithm, our proposed denoising DAE provided superior performance on the three objective evaluations.												
Intelligent Music Production	Level Adjustment	2012	Dukes, Bob	Implementation and Evaluation of Autonomous Multi-track Fader Control	http://www.aes.org/e-lib/browse.cfm?elib=16226	A new approach to the autonomous control of faders for multi-track audio mixing is presented. The algorithm is designed to generate an automatic sound mix from an arbitrary number of monaural or stereo audio tracks of any sample rate, and to be suitable for both live and post-production use. Mixing levels are determined by the use of the EBU R-128 loudness measure, with a cross-adaptive process to bring each track to a time-varying average. A hysteresis loudness gate and selective smoothing prevents the adjustment of intentional dynamics in the music. Realtime and off-line software implementations have been created. Subjective evaluation is provided in the form of listening tests, where the method is compared against the results of a human mix and a previous automatic fader implementation.												
Intelligent Music Production	Level Adjustment	2012	Ward, Dominic; Reiss, Joshua D; Athwal, Cham	Multi-track mixing using a model of loudness and partial loudness	http://www.aes.org/e-lib/browse.cfm?elib=16436	A method for generating a mix of multi-track recordings using an auditory model has been developed. The proposed method is based on the concept that a balanced mix is one in which the loudness of all instruments are equal. A sophisticated psychoacoustic loudness model is used to measure the loudness of each track both in quiet and when mixed with any combination of the remaining tracks. Such measures are used to control the track gains in a time-varying manner. Finally we demonstrate how model predictions of partial loudness can be used to counteract energetic masking for any track, allowing the user to achieve better channel intelligibility in complex music mixtures.												
Intelligent Music Production	Feature	2011	Scott, Jeffrey; Kim, Youngmoo E	ANALYSIS OF ACOUSTIC FEATURES FOR AUTOMATED MULTI-TRACK MIXING	https://archives.ismir.net/ismir2011/paper/000027.pdf	The capability of the average person to generate digital music content has rapidly expanded over the past several decades. While the mechanics of creating a multi-track recording are relatively straightforward, using the available tools to create professional quality work requires substantial training and experience. We address one of the most fundamental processes to creating a ﬁnished product, namely determining the relative gain levels of each track to produce a ﬁnal, mixed song. By modeling the time-varying mixing coefﬁcients with a linear dynamical system, we train models that predict a weight vector for a given instrument using features extracted from the audio content of all of the tracks.			RockBand Database	https://rbdb.online/					Demo	http://music.ece.drexel.edu/research/AutoMix		
Intelligent Music Production	General Mixing	2011	Scott, Jeffrey; Prockup, Matthew; Schmidt, Erik M; Kim, Youngmoo E	AUTOMATIC MULTI-TRACK MIXING USING LINEAR DYNAMICAL SYSTEMS	http://music.ece.drexel.edu/files/Navigation/Publications/Scott2011a.pdf	Over the past several decades music production has evolved from something that was only possible with multiroom, multi-million dollar studios into the province of the average person’s living room. New tools for digital production have revolutionized the way we consume and interact with music on a daily basis. We propose a system based on a structured audio framework that can generate a basic mix-down of a set of multi-track audio ﬁles using parameters learned through supervised machine learning. Given the new surge of mobile content consumption, we extend this system to operate on a mobile device as an initial measure towards an integrated interactive mixing platform for multi-track music.			RockBand Database	https://rbdb.online/					Demo	http://music.ece.drexel.edu/research/AutoMix/lds		
Intelligent Music Production	Equalization	2011	Reiss, Joshua D.	Design of Audio Parametric Equalizer Filters Directly in the Digital Domain	http://ieeexplore.ieee.org/document/5629354/	Most design procedures for a digital parametric equalizer begin with analog design techniques, followed by applying the bilinear transform to an analog prototype. As an alternative, an approximation to the parametric equalizer is sometimes designed using pole-zero placement techniques. In this paper, we present an exact derivation of the parametric equalizer without resorting to an analog prototype. We show that there are many solutions to the parametric equalizer design constraints as usually stated, but only one of which consistently yields stable, minimum phase bbehaviorwith the upper and lower cutoff frequencies positioned around the center frequency. The conditions for complex conjugate poles and zeros are found and the resultant pole zero placements are examined.												
Intelligent Music Production	Summary	2011	Reiss, Joshua D.	Intelligent systems for mixing multichannel audio	http://ieeexplore.ieee.org/document/6004988/	Multichannel signal processing techniques are usually concerned with extracting information about sources from several received signals. In this paper, we describe an emerging field of multichannel audio signal processing where the inter-channel relationships are exploited in order to manipulate the multichannel content. Applications to realtime, automatic audio production are described and the necessary technologies and the architecture of such systems are presented. The current state of the art is reviewed, and directions of future research are also discussed.												
Intelligent Music Production	Level Adjustment	2011	Simpson, Andrew J R; Reiss, Joshua D	The Effect of Loudness Overflow on Equal- Loudness-Level Contours	http://www.aes.org/e-lib/browse.cfm?elib=15834	This paper presents a formal derivation of the Loudness Overflow Effect (LOE), which describes the impact of nonlinear distortion on loudness. Computational analysis is then performed, comprised of two experiments involving two compressive static nonlinearities, and using two well-known time-varying loudness models. The results characterize the nonlinearities in terms of LOE as a function of frequency and of listening level in the case of 250ms pure-tone stimuli, and in terms of the traditional equal-loudness-level contours. The analysis is then extended to synthesized wind instruments for one of the nonlinearities. The effect of the nonlinearity on loudness as a function of musical note fundamental frequency and listening level is described for various synthesized instruments.												
Intelligent Music Production	General Mixing	2010	Gang, Ren; Bocko, Gregory; Lundberg, Justin; Headlam, Dave; Bocko, Mark F	AUTOMATIC MUSIC PRODUCTION SYSTEM EMPLOYING PROBABILISTIC EXPERT SYSTEMS	http://www.aes.org/e-lib/browse.cfm?elib=15677	An automatic music production system based on expert audio engineering knowledge is proposed. An expert system based on a probabilistic graphical model is employed to embed professional audio engineering knowledge and infer automatic production decisions based on musical information extracted from audio files. The production pattern, which is represented as probabilistic graphical model, can be ‘learned’ from the operation data of a human audio engineer or manually constructed from domain knowledge. The authors also discuss the real-time implementation of the proposed automatic production system for live mixing application scenarios. Musical event alignment and prediction algorithms are introduced to improve the time synchronization performance of our production model. The authors conclude with performance evaluations and a brief summary.												
Sound Source Separation	Music Audio Source Separation	2010	Salamon, Justin; Gomez, Emilia	Melody Extraction from Polyphonic Music Signals using Pitch Contour Characteristics	http://www.mtg.upf.edu/system/files/publications/SalamonGomezMelodyTASLP2012.pdf	We present a novel system for the automatic extraction of the main melody from polyphonic music recordings. Our approach is based on the creation and characterisation of pitch contours, time continuous sequences of pitch candidates grouped using auditory streaming cues. We deﬁne a set of contour characteristics and show that by studying their distributions we can devise rules to distinguish between melodic and non-melodic contours. This leads to the development of new voicing detection, octave error minimisation and melody selection techniques.	Source Code	https://www.music-ir.org/mirex/wiki/MIREX_HOME							Demo	https://www.music-ir.org/mirex/wiki/MIREX_HOME		
Intelligent Music Production	Equalization	2009	Perez, Enrique; Reiss, Joshua	Automatic equalization of multi-channel audio using cross-adaptive methods	http://www.aes.org/e-lib/browse.cfm?elib=15026	A method for automatically equalizing a multi-track mixture has been implemented. The method aims to achieve equal average perceptual loudness on all frequencies amongst all multi-track channels. The method uses accumulative spectral decomposition techniques together with cross-adaptive audio effects to achieve equalization. The method has applications in live and recorded audio mixing where the audio engineer would like to reduce set-up time, or as a tool for inexperienced users wishing to perform audio mixing. Results are reported which show how the frequency content of each channel is modified, and which demonstrate the ability of the automatic equalization method to achieve a well-balanced and equalized final mix.												
Intelligent Music Production	Level Adjustment	2009	Perez-Gonzalez, Enrique; Reiss, Joshua	Automatic gain and fader control for live mixing	http://ieeexplore.ieee.org/document/5346498/	A cross-adaptive mixing device has been developed for the purpose of optimizing the gain levels of a live audio mixture. The method aims to achieve optimal mixing levels by optimizing the ratios between the loudness of each individual input channel and the overall loudness contained in a stereo mix. In order to evaluate the amount of loudness of each channel in real-time, accumulative statistical measurements were performed. The system uses a cross-adaptive algorithm to map the loudness indicators to the channel gain values. The system has applications in automatic mixing of live music, live mixing of game audio, and studio recording post-production.									Demo	http://www.elec.qmul.ac.uk/digitalmusic/automaticmixin		
Intelligent Music Production	Equalization	2009	Barchiesi, Daniele; Reiss, Josh	Automatic Target Mixing using Least-Squares Optimization of Gains and Equalization Settings	https://www.eecs.qmul.ac.uk/~josh/documents/2009/BarchiesiReiss-AutomaticTargetMixing.pdf	The proposed automatic target mixing algorithm determines the gains and the equalization settings for the mixing of a multi-track recording using a least-squares optimization. These parameters are estimated using a single channel target mix, that is a signal which contains the same audio tracks as the multi-track recording, but that has been previously mixed using some unknown settings.												
Music Information Retrieval	Beat Detection	2009	Grosche, Peter; Muller, Meinard	Computing predominant local periodicity information in music recordings	http://ieeexplore.ieee.org/document/5346544/	The periodic structure of musical events plays a crucial role in the perception of tempo as well as the sensation of note changes and onsets. In this paper, we introduce a novel function that reveals the predominant local periodicity (PLP) in music recordings. Here, our main idea is to estimate for each time position a periodicity kernel that best explains the local periodic nature of previously extracted note onset information and then to accumulate all these kernels to yield a single function. This function, which is also referred to as PLP curve, reveals musically meaningful periodicity information even for non-percussive music with soft and blurred note onsets. Such information is useful not only for stabilizing note onset detection but also for beat tracking and tempo estimation in the case of music with changing tempo.			RWC Music Database	https://staff.aist.go.jp/m.goto/RWC-MDB/								
Music Information Retrieval	Chord Detection	2009	Stark, Adam M	Real-Time Chord Recognition for Live Performance	http://eecs.qmul.ac.uk/~markp/2009/StarkPlumbley09-icmc.pdf	This paper describes work aimed at creating an efﬁcient, real-time, robust and high performance chord recognition system for use on a single instrument in a live performance context. An improved chroma calculation method is combined with a classiﬁcation technique based on masking out expected note positions in the chromagram and minimising the residual energy. We demonstrate that our approach can be used to classify a wide range of chords, in real-time, on a frame by frame basis. We present these analysis techniques as externals for Max/MSP.	Source Code	http://www.elec.qmul.ac.uk/digitalmusic/people/adams/chordrec/										
Intelligent Music Production	Level Adjustment	2008	Gonzalez, Enrique Perez; Reiss, Joshua	An automatic maximum gain normalization technique with applications to audio mixing.	http://www.aes.org/e-lib/browse.cfm?elib=14541	A method for real-time magnitude gain normalization of a changing linear system has been developed and tested with a parametric filter design. The method is useful in situations where the maximum gain before feedback is needed. The method automatically calculates the appropriate gain that should be applied in order to maintain maximum unitary gain. The method uses an impulse measurement of a mathematical model of the system to be normalized. This is particularly useful for mixing engineers, who have to continually revise their gain structure in order to maximize gain before feedback. The system is also useful in many other situations where solving the analytical solution from the mathematical model is not possible.												
Music Information Retrieval	Chord Detection	2008	Heng-Tze Cheng; Yi-Hsuan Yang; Yu-Ching Lin; I-Bin Liao; Chen, Homer H.	Automatic chord recognition for music classification and retrieval	http://ieeexplore.ieee.org/document/4607732/	As one of the most important mid-level features of music, chord contains rich information of harmonic structure that is useful for music information retrieval. In this paper, we present a chord recognition system based on the N-gram model. The system is time-efficient, and its accuracy is comparable to existing systems. We further propose a new method to construct chord features for music emotion classification and evaluate its performance on commercial song recordings. Experimental results demonstrate the advantage of using chord features for music classification and retrieval.												
Intelligent Music Production	Time Adjustment	2008	Gonzalez, Enrique Perez; Reiss, Joshua	Determination and correction of individual channel time offsets for signals involved in an audio mixture	http://www.aes.org/e-lib/browse.cfm?elib=14782	A method for reducing comb-filtering effects due to delay time differences between audio signals in a sound mixer has been implemented. The method uses a multi-channel cross-adaptive effect topology to automatically determine the minimal delay and polarity contributions required to optimize the sound mixture. The system uses real time, time domain transfer function measurements to determine and correct the individual channel offset for every signal involved in the audio mixture. The method has applications in live and recorded audio mixing where recording a single sound source with more than one signal path is required, for example when recording a piano with multiple microphones. Results are reported which determine the effectiveness of the proposed method.												
Text Book	Text Book	2008	Wiley, A John	Digital Audio Signal Processing	https://www.wiley.com/en-us/Digital+Audio+Signal+Processing%2C+2nd+Edition-p-9780470997857													
Music Information Retrieval	Beat Detection	2007	Zenz, Veronika; Rauber, Andreas	Automatic Chord Detection Incorporating Beat and Key Detection	http://ieeexplore.ieee.org/document/4728534/	We introduce an algorithm that analyzes audio signals to extract chord-sequence information. The main goal of our approach lies in incorporating music theoretical knowledge without restricting the input data to a narrow range of musical styles. At the basis of our approach lies pitch detection using enhanced autocorrelation, supported by key detection and beat tracking. The Chords themselves are identiﬁed by comparing generated and reference Pitch Class Proﬁles. A smoothing algorithm is applied to the chordsequence which optimizes the number of chord changes and thus takes into consideration the comparatively stable nature of chords. In this paper we present an evaluation performed on a large set of 35 pieces of diverse music showing an average performance of 65% accuracy.			RWC Music Database	https://staff.aist.go.jp/m.goto/RWC-MDB/								
Intelligent Music Production	Panning	2007	Gonzalez, Enrique Perez; Reiss, Joshua	AUTOMATIC MIXING: LIVE DOWNMIXING STEREO PANNER TEMPLATES FOR DAFX04, NAPLES, ITALY	https://www.semanticscholar.org/paper/AUTOMATIC-MIXING%3A-LIVE-DOWNMIXING-STEREO-PANNER-FOR-Gonzalez-Reiss/7057648eca7ba75172d851025e1390c5dd898c8b	An automatic stereo panning algorithm intended for live multitrack downmixing has been researched. The algorithm uses spectral analysis to determine the panning position of sources. The method uses filter bank quantitative channel dependence, priority channel architecture and constrained rules to assign panning criteria. The algorithm attempts to minimize spectral masking by allocating similar spectra to different panning spaces. The algorithm has been implemented; results on its convergence, automatic panning space allocation, and left-right inter-channel phase relationship are presented.			BASS-dB database	http://bass-db.gforge.inria.fr/BASS-dB/								
Sound Source Separation	Evaluation	2006	Vincent, E.; Gribonval, R.; Fevotte, C.	Performance measurement in blind audio source separation	http://ieeexplore.ieee.org/document/1643671/	In this article, we discuss the evaluation of Blind Audio Source Separation (BASS) algorithms. Depending on the exact application, different distortions can be allowed between an estimated source and the wanted true source. We consider four different sets of such allowed distortions, from time-invariant gains to time-varying ﬁlters. In each case we decompose the estimated source into a true source part plus error terms corresponding to interferences, additive noise and algorithmic artifacts. Then we derive a global performance measure using an energy ratio, plus a separate performance measure for each error term. These measures are computed and discussed on the results of several BASS problems with various difﬁculty levels.			BASS-dB	http://bass-db.gforge.inria.fr/BASS-dB/					Demo	http://www.irisa.fr/metiss/home_html-fr/switchLanguage%3Fset_language=en.html		
Music Information Retrieval	Instrument Classification	2006	Sell, Greg; Mysore, Gautham J; Chon, Song Hui	Musical Instrument Detection Detecting instrumentation in polyphonic musical signals on a frame-by-frame basis	http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.142.8908	Knowledge of the instrumentation of a musical signal at any given time could be useful for major audio signal processing problems such as sound source separation and automated music transcription. Knowing which instruments are playing is a first step toward more intelligently designed solutions to these very important and largely unsolved challenges. So, in this paper, we attempt the problem of identifying the instrumentation of a musical signal at any given time using several machine learning techniques (logistic regression, K-NN, SVM). We approached the problem as a series of separate binary classifications (as opposed to a multivariate problem) so that we could mix and match the best algorithm for each instrument to create the best overall classifier.												
Music Information Retrieval	Beat Detection	2004	Leveau, Pierre; Daudet, Laurent; Richard, Gael	METHODOLOGY AND TOOLS FOR THE EVALUATION OF AUTOMATIC ONSET DETECTION ALGORITHMS IN MUSIC	https://www.semanticscholar.org/paper/Methodology-and-Tools-for-the-evaluation-of-onset-Daudet-Richard/c49d6291ed2d4392cc14b08ad580d304f2a7b21f	This paper addresses the problem of the performance evaluation of algorithms for the automatic detection of note onsets in music signals. Our experiments show that creating a database of reference ﬁles with reliable humanannotated onset times is a complex task, since its subjective part cannot be neglected. This work provides a methodology to construct such a database. With the use of a carefully designed software tool, called SOL (Sound Onset Labellizer), we can obtain a set of reference onset times that are cross-validated amongst different expert listeners. We show that the mean error of annotated times across test subjects is very much signal-dependent. This value can be used, when evaluating automatic labelling, as an indication of the relevant tolerance window. The SOL annotation software is to be released freely for research purposes. Our test library, 17 short sequences containing about 750 onsets, comes from copyright-free music or from the public RWC database. The corresponding validated onset labels are also freely distributed, and are intended to form the starting point for the deﬁnition of a reliable benchmark.			RWC Music Database	https://staff.aist.go.jp/m.goto/RWC-MDB/	Database and tools for onset detection evaluation	http://www.enst.fr/˜grichard/ISMIR04/						
Data Set	Data Set	2003	Goto, Masataka	RWC Music Database: Music Genre Database and Musical Instrument Sound Database	https://pdfs.semanticscholar.org/ae8c/a8c76b9f74fb9dafe03a6b62d11998019fc1.pdf	This paper describes the design policy and speciﬁcations of the RWC Music Database, a copyright-cleared music database (DB) compiled speciﬁcally for research purposes. Shared DBs are common in other research ﬁelds and have made signiﬁcant contributions to progress in those ﬁelds. The ﬁeld of music information processing, however, has lacked a common DB of musical pieces or a large-scale DB of musical instrument sounds. We therefore recently constructed the RWC Music Database comprising four original component DBs: Popular Music Database (100 pieces), Royalty-Free Music Database (15 pieces), Classical Music Database (50 pieces), and Jazz Music Database (50 pieces). In this paper we report the construction of two additional component DBs: Music Genre Database (100 pieces) and Musical Instrument Sound Database (50 instruments). It is our hope that our DB will make a signiﬁcant contribution to future advances in the ﬁeld of music information processing.			RWC Music Database	https://staff.aist.go.jp/m.goto/RWC-MDB/								
Sound Source Separation	Summary	2001	Lee, Daniel D; Seung, H Sebastian	Algorithms for Non-negative Matrix Factorization	https://papers.nips.cc/paper/1861-algorithms-for-non-negative-matrix-factorization.pdf	Non-negative matrix factorization (NMF) has previously been shown to be a useful decomposition for multivariate data. Two different multiplicative algorithms for NMF are analyzed. They differ only slightly in the multiplicative factor used in the update rules. One algorithm can be shown to minimize the conventional least squares error while the other minimizes the generalized Kullback-Leibler divergence. The monotonic convergence of both algorithms can be proven using an auxiliary function analogous to that used for proving convergence of the ExpectationMaximization algorithm. The algorithms can also be interpreted as diagonally rescaled gradient descent, where the rescaling factor is optimally chosen to ensure convergence.												
Music Information Retrieval	Instrument Classification	2001	Eronen, Antti	AUTOMATIC MUSICAL INSTRUMENT RECOGNITION	http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.79.6635&rep=rep1&type=pdf	N.A.												
Intelligent Music Production	Evolutional computation	2001	Takagi, H.	Interactive evolutionary computation: fusion of the capabilities of EC optimization and human evaluation	http://ieeexplore.ieee.org/document/949485/	In this paper, we survey the research on interactive evolutionary computation (IEC). The IEC is an EC that optimizes systems based on subjective human evaluation. The deﬁnition and features of the IEC are ﬁrst described and then followed by an overview of the IEC research. The overview primarily consists of application research and interface research. In this survey, the IEC application ﬁelds include graphic arts and animation, 3-D CG lighting, music, editorial design, industrial design, facial image generation, speech processing and synthesis, hearing aid ﬁtting, virtual reality, media database retrieval, data mining, image processing, control and robotics, food industry, geophysics, education, entertainment, social system, and so on. Also in this survey, the interface research to reduce human fatigue includes improving ﬁtness input interfaces and displays based on ﬁtness prediction, accelerating EC convergence especially in early EC generations, examining combinations of interactive and normal EC, and investigating active user intervention. Finally, we discuss the IEC from the point of the future research direction of computational intelligence. In order to show the status quo IEC research, this paper primarily features a survey of about 250 IEC research papers rather than a carefully selected representation of a few papers.												
Music Information Retrieval	Genre Classification	2001	Pachet, F.; Westermann, G.; Laigre, D.	Musical data mining for electronic music distribution	https://www.researchgate.net/publication/3939810_Musical_data_mining_for_electronic_music_distribution	Music classification is a key ingredient for electronic music distribution. Because of the lack of standards in music classification – or the lack of enforcement of existing standards – there is a huge amount of unclassified titles of music in the world. In this paper we propose a method of classification based on musical data mining techniques that uses co-occurrence and correlation analysis for classification. This method allows for the rapid extraction of similarities between musical titles or artists. We investigate the usability of radio playlists and compilation CD databases for our data mining technique, and we compare the generated results with human similarity judgments. Based on a clustering technique, we show that interesting clusters can reveal specific music genres and allow classifying titles of music in a kind of objective manner.												
