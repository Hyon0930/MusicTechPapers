#  [Representations of Sound in Deep Learning of Audio Features from Music](https://arxiv.org/abs/1712.02898)
**Author**: Shuvaev, Sergey; Giaffar, Hamza; Koulakov, Alexei A

**Year**: 2017
>**Abstract**: The work of a single musician, group or composer can vary widely in terms of musical style. Indeed, different stylistic elements, from performance medium and rhythm to harmony and texture, are typically exploited and developed across an artist’s lifetime. Yet, there is often a discernable character to the work of, for instance, individual composers at the perceptual level – an experienced listener can often pick up on subtle clues in the music to identify the composer or performer. Here we suggest that a convolutional network may learn these subtle clues or features given an appropriate representation of the music. In this paper, we apply a deep convolutional neural network to a large audio dataset and empirically evaluate its performance on audio classification tasks. Our trained network demonstrates accurate performance on such classification tasks when presented with ~ 5 s examples of music obtained by simple transformations of the raw audio waveform. A particularly interesting example is the spectral representation of music obtained by application of a logarithmically spaced filter bank, mirroring the early stages of auditory signal transduction in mammals. The most successful representation of music to facilitate discrimination was obtained via a random matrix transform (RMT). Networks based on logarithmic filter banks and RMT were able to correctly guess the one composer out of 31 possibilities in 68% and 84% of cases respectively.

**Data Set**: Not availabe

**Source Code**: Not availabe

**Demo**: Not availabe

#  [Variation in Multitrack Mixes: Analysis of Low-level Audio Signal Features](http://www.aes.org/e-lib/browse.cfm?elib=18332)
**Author**: Wilson, Alex; Fazenda, Bruno

**Year**: 2016
>**Abstract**: To further the development of intelligent music production tools towards generating mixes that would realistically be created by a human mix-engineer, it is important to understand what kind of mixes can be created, and are typically created, by human mix-engineers. This paper presents an analysis of 1501 mixes, over 10 different songs, created by mix-engineers. The primary dimensions of variation in the full dataset of mixes were “amplitude,” “brightness,” “bass,” and “width” as determined by feature-extraction and subsequent principal component analysis. The distribution of representative features approximated a normal distribution and this is then used to obtain general trends and tolerance bounds for these features. The results presented here are useful as parametric guidance for intelligent music production systems.

**Data Set**: [Cambridge Multitracks](http://www.cambridge-mt.com)

**Source Code**: Not availabe

**Demo**: Not availabe

#  [An Evaluation of Audio Feature Extraction Toolboxes](https://www.ntnu.edu/documents/1001201110/1266017954/DAFx-15_submission_43_v2.pdf)
**Author**: Moffat, David; Ronan, David; Reiss, Joshua D

**Year**: 2015
>**Abstract**: Audio feature extraction underpins a massive proportion of audio processing, music information retrieval, audio effect design and audio synthesis. Design, analysis, synthesis and evaluation often rely on audio features, but there are a large and diverse range of feature extraction tools presented to the community. An evaluation of existing audio feature extraction libraries was undertaken. Ten libraries and toolboxes were evaluated with the Cranﬁeld Model for evaluation of information retrieval systems, reviewing the coverage, effort, presentation and time lag of a system. Comparisons are undertaken of these tools and example use cases are presented as to when toolboxes are most suitable. This paper allows a software engineer or researcher to quickly and easily select a suitable audio feature extraction toolbox.

**Data Set**: [The million song dataset](http://millionsongdataset.com/)

**Source Code**: Not availabe

**Demo**: Not availabe

